{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d21ae-9e31-4e47-80f8-255fb231e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: setup & utilities ---\n",
    "import os, json, time, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "CACHE_ROOT = Path(\"data/raw\")\n",
    "CACHE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"ift6758-course/1.0\"}\n",
    "TIMEOUT = 15\n",
    "THROTTLE_SEC = 0.10\n",
    "MAX_WORKERS = 8\n",
    "MAX_RETRIES = 3\n",
    "BACKOFF_BASE = 0.5\n",
    "\n",
    "GAME_TYPE_MAP = {\n",
    "    \"preseason\": \"01\",\n",
    "    \"regular\": \"02\",\n",
    "    \"playoffs\": \"03\",\n",
    "    \"allstar\": \"04\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_game_id(season_start_year: int, game_type: str, game_number: int) -> str:\n",
    "    \"\"\"\n",
    "    Build a unique NHL GAME_ID string.\n",
    "\n",
    "    Format:\n",
    "        GAME_ID = YYYYTTNNNN\n",
    "            YYYY : season start year (e.g., 2016 for the 2016–17 season)\n",
    "            TT   : game type code\n",
    "                   \"01\" = preseason\n",
    "                   \"02\" = regular season\n",
    "                   \"03\" = playoffs\n",
    "                   \"04\" = all-star\n",
    "            NNNN : four-digit sequential game number\n",
    "\n",
    "    Args:\n",
    "        season_start_year (int): The starting year of the season.\n",
    "        game_type (str): One of {'preseason', 'regular', 'playoffs', 'allstar'}.\n",
    "        game_number (int): Sequential number of the game.\n",
    "\n",
    "    Returns:\n",
    "        str: The constructed GAME_ID.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `game_type` is not one of the supported keys.\n",
    "    \"\"\"\n",
    "    if game_type not in GAME_TYPE_MAP:\n",
    "        raise ValueError(f\"Unknown game_type '{game_type}'\")\n",
    "    return f\"{season_start_year}{GAME_TYPE_MAP[game_type]}{game_number:04d}\"\n",
    "\n",
    "\n",
    "def cache_path(cache_root: Path, game_id: str) -> Path:\n",
    "    \"\"\"\n",
    "    Build the local cache file path for a given GAME_ID.\n",
    "\n",
    "    Args:\n",
    "        cache_root (Path): Root directory of the cache.\n",
    "        game_id (str): The NHL GAME_ID.\n",
    "\n",
    "    Returns:\n",
    "        Path: Full path to the cached JSON file,\n",
    "              e.g. data/raw/2016/type-02/2016020001.json\n",
    "    \"\"\"\n",
    "    season = game_id[:4]\n",
    "    gtype = game_id[4:6]\n",
    "    return cache_root / season / f\"type-{gtype}\" / f\"{game_id}.json\"\n",
    "\n",
    "\n",
    "def http_get_with_retries(url: str, timeout: float = 15.0) -> Optional[requests.Response]:\n",
    "    \"\"\"\n",
    "    Perform an HTTP GET request with retry and exponential backoff.\n",
    "\n",
    "    Retries are triggered for network errors, HTTP 5xx, or HTTP 429.\n",
    "    Returns None immediately for a 404 (resource not found).\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to fetch.\n",
    "        timeout (float, optional): Timeout (seconds) for each request attempt.\n",
    "\n",
    "    Returns:\n",
    "        Optional[requests.Response]: The successful response object,\n",
    "            or None if a 404 is received or all retries fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "            if r.status_code == 404:\n",
    "                return None\n",
    "            if r.status_code >= 500 or r.status_code == 429:\n",
    "                time.sleep(BACKOFF_BASE * (2 ** attempt))\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except requests.RequestException:\n",
    "            time.sleep(BACKOFF_BASE * (2 ** attempt))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b868e-4af4-440a-9769-eee5e6a6f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: enumerate valid game_ids ---\n",
    "def list_game_ids_for_season(season_start_year: int, game_type: str, max_games_hint: int = 1500) -> List[str]:\n",
    "    valid_ids = []\n",
    "    miss = 0\n",
    "    for n in tqdm(range(1, max_games_hint + 1), desc=f\"{season_start_year} {game_type}\"):\n",
    "        gid = build_game_id(season_start_year, game_type, n)\n",
    "        url = f\"https://api-web.nhle.com/v1/gamecenter/{gid}/play-by-play\"\n",
    "        r = http_get_with_retries(url, timeout=TIMEOUT)\n",
    "        if r is None:\n",
    "            miss += 1\n",
    "            if miss >= 120:\n",
    "                break\n",
    "        else:\n",
    "            valid_ids.append(gid)\n",
    "            miss = 0\n",
    "        time.sleep(THROTTLE_SEC)\n",
    "    print(f\" {season_start_year} {game_type}: found {len(valid_ids)} valid games.\")\n",
    "    return valid_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633211e-e9c4-4d3b-bd6b-6573989007d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: fetch & parallel download ---\n",
    "def fetch_one(game_id: str) -> bool:\n",
    "    p = cache_path(CACHE_ROOT, game_id)\n",
    "    if p.exists():\n",
    "        return True\n",
    "    url = f\"https://api-web.nhle.com/v1/gamecenter/{game_id}/play-by-play\"\n",
    "    r = http_get_with_retries(url, timeout=TIMEOUT)\n",
    "    if r is None:\n",
    "        return False\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(json.dumps(r.json()), encoding=\"utf-8\")\n",
    "    time.sleep(THROTTLE_SEC)\n",
    "    return True\n",
    "\n",
    "def fetch_season_fast(season_start_year: int, game_type: str):\n",
    "    game_ids = list_game_ids_for_season(season_start_year, game_type)\n",
    "    ok = 0\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futures = [ex.submit(fetch_one, gid) for gid in game_ids]\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=f\"Downloading {season_start_year} {game_type}\"):\n",
    "            try:\n",
    "                ok += 1 if f.result() else 0\n",
    "            except Exception as e:\n",
    "                print(\"[ERR]\", e)\n",
    "    print(f\" {season_start_year} {game_type}: downloaded {ok}/{len(game_ids)} games.\")\n",
    "    return ok\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d30d31-238d-46ed-9b59-e0e01013e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_year_range_fast(start_year: int, end_year: int,\n",
    "                          types=(\"regular\", \"playoffs\")) -> dict:\n",
    "    \"\"\"\n",
    "    Download NHL play-by-play data for multiple seasons and game types in batch.\n",
    "\n",
    "    This function iterates over each season and game type, calling\n",
    "    `fetch_season_fast()` to download all available game JSONs between the given\n",
    "    start and end years.\n",
    "\n",
    "    Args:\n",
    "        start_year (int): First season to include (e.g., 2016 for the 2016–17 season).\n",
    "        end_year (int): Last season to include (inclusive).\n",
    "        types (tuple[str]): Sequence of game types to fetch, usually\n",
    "            (\"regular\", \"playoffs\").\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, str], int]:\n",
    "            A dictionary mapping (season_start_year, game_type) →\n",
    "            number of successfully downloaded (or cached) games.\n",
    "\n",
    "    Notes:\n",
    "        - Each call to `fetch_season_fast()` handles retries and caching.\n",
    "        - Downloads may take significant time; consider testing first\n",
    "          with a short range (e.g. 2016–2017).\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for gtype in types:\n",
    "            print(f\"\\n=== {year} {gtype} ===\")\n",
    "            try:\n",
    "                ok = fetch_season_fast(year, gtype)\n",
    "                results[(year, gtype)] = ok\n",
    "            except Exception as e:\n",
    "                print(f\"[ERR] {year} {gtype}: {e}\")\n",
    "    return results\n",
    "\n",
    "fetch_year_range_fast(2016, 2024, types=(\"regular\", \"playoffs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9629d8-945d-43f7-b4d6-c4600456d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: extraction to DataFrame ---\n",
    "import pandas as pd\n",
    "\n",
    "def extract_events(json_obj: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract play-by-play events from a single NHL game JSON file.\n",
    "\n",
    "    Supports the newer API structure (api-web.nhle.com), where events\n",
    "    are stored in the top-level key `\"plays\"`.\n",
    "\n",
    "    Args:\n",
    "        json_obj (dict): Parsed JSON object representing a single game.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A table of events with the following columns:\n",
    "            ['event', 'secondaryType', 'period', 'periodTime',\n",
    "             'dateTime', 'team', 'x', 'y']\n",
    "\n",
    "        The DataFrame may be empty if no plays are found.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    plays = json_obj.get(\"plays\", None)\n",
    "    if isinstance(plays, list) and len(plays) > 0:\n",
    "        for p in plays:\n",
    "            details = p.get(\"details\", {}) or {}\n",
    "            team    = p.get(\"team\", {}) or {}\n",
    "            periodD = p.get(\"periodDescriptor\", {}) or {}\n",
    "\n",
    "            rows.append({\n",
    "                \"event\":         p.get(\"typeDescKey\") or p.get(\"typeCode\"),\n",
    "                \"secondaryType\": details.get(\"shotType\") or details.get(\"eventCode\"),\n",
    "                \"period\":        periodD.get(\"number\"),\n",
    "                \"periodTime\":    p.get(\"timeInPeriod\") or details.get(\"timeInPeriod\"),\n",
    "                \"dateTime\":      p.get(\"timeUTC\") or details.get(\"eventOwnerTeamTime\") or None,\n",
    "                \"team\":          team.get(\"name\") or team.get(\"triCode\"),\n",
    "                \"x\":             details.get(\"xCoord\"),\n",
    "                \"y\":             details.get(\"yCoord\"),\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Return an empty DataFrame with the expected schema if no data\n",
    "    return pd.DataFrame(columns=[\n",
    "        \"event\", \"secondaryType\", \"period\", \"periodTime\",\n",
    "        \"dateTime\", \"team\", \"x\", \"y\"\n",
    "    ])\n",
    "\n",
    "\n",
    "# --- replace your build_dataset with this version ---\n",
    "def build_dataset(start: int = 2016, end: int = 2024, gtype: str = \"regular\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a combined play-by-play dataset by concatenating multiple cached game files.\n",
    "\n",
    "    Adds two columns for per-game navigation:\n",
    "      - game_id   (from filename)\n",
    "      - game_type (from folder: type-02 -> regular, type-03 -> playoffs)\n",
    "    \"\"\"\n",
    "    root = CACHE_ROOT\n",
    "    tcode = GAME_TYPE_MAP[gtype]\n",
    "    all_dfs = []\n",
    "\n",
    "    # reverse map for readability\n",
    "    rev_type = {\"01\": \"preseason\", \"02\": \"regular\", \"03\": \"playoffs\", \"04\": \"allstar\"}\n",
    "\n",
    "    for year in range(start, end + 1):\n",
    "        d = root / str(year) / f\"type-{tcode}\"\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for f in d.glob(\"*.json\"):\n",
    "            try:\n",
    "                data = json.loads(f.read_text())\n",
    "                df = extract_events(data)\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                # derive fields from path\n",
    "                game_id = f.stem\n",
    "                type_code = f.parent.name.replace(\"type-\", \"\")  # \"02\"/\"03\"\n",
    "                game_type = rev_type.get(type_code, type_code)\n",
    "                df[\"game_id\"] = game_id\n",
    "                df[\"game_type\"] = game_type\n",
    "                all_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(\"[WARN]\", f, e)\n",
    "\n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"event\",\"secondaryType\",\"period\",\"periodTime\",\"dateTime\",\"team\",\"x\",\"y\",\n",
    "            \"game_id\",\"game_type\"\n",
    "        ])\n",
    "    return pd.concat(all_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f1fc5-a4d9-414e-9310-d185dddcfa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "cands = list((CACHE_ROOT / \"2016\" / \"type-02\").glob(\"*.json\"))\n",
    "print(\"# Files：\", len(cands))\n",
    "sample = json.loads(cands[0].read_text())\n",
    "\n",
    "df_test = extract_events(sample)\n",
    "print(\"Colums：\", df_test.columns.tolist())\n",
    "print(\"Shape：\", df_test.shape)\n",
    "df_test.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef425a-27bc-4bd7-b6ff-cf404176d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: save clean dataset ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def save_clean_dataset(df: pd.DataFrame, out_path: Path):\n",
    "    \"\"\"\n",
    "    Save a cleaned play-by-play dataset to disk in CSV or compressed CSV (gzip) format.\n",
    "\n",
    "    The function automatically creates parent directories if they do not exist,\n",
    "    and determines compression based on the file extension.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "        out_path (Path): Destination file path. Compression is inferred:\n",
    "            - Ends with \".gz\" or \".csv.gz\" → saved with gzip compression\n",
    "            - Otherwise → saved as plain CSV\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.suffix == \".gz\" or out_path.suffixes[-2:] == [\".csv\", \".gz\"]:\n",
    "        df.to_csv(out_path, index=False, compression=\"gzip\")\n",
    "    else:\n",
    "        df.to_csv(out_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9c59d-7e13-446e-8da6-04c6470d801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = build_dataset(2016, 2023, \"regular\")\n",
    "print(\"df_small shape:\", df_small.shape)\n",
    "assert not df_small.empty, \"Dataset is empty, please double check.\"\n",
    "\n",
    "outp = Path(\"data/clean/events_regular_2016-2023.csv.zip\")\n",
    "save_clean_dataset(df_small, outp)\n",
    "\n",
    "print(\"Output:\", outp.resolve(), \"size(bytes)=\", outp.stat().st_size)\n",
    "assert outp.exists() and outp.stat().st_size > 0, \"Output is empty or does not exist\"\n",
    "\n",
    "# Read back and compare\n",
    "df_back = pd.read_csv(outp, compression=\"zip\")\n",
    "print(\"Read-back shape:\", df_back.shape)\n",
    "print(\"Columns:\", list(df_back.columns))\n",
    "assert df_back.shape[0] == df_small.shape[0], \"Data does not match\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2ca86-25e7-4fc2-9805-1bd9ad2cc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Quick QA + Plot ---\n",
    "# (Run only after building df_reg / df_po)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def quick_event_counts(df: 'pd.DataFrame', topn: int = 10):\n",
    "    \"\"\"\n",
    "    Display the most frequent play-by-play event types in a dataset.\n",
    "\n",
    "    Prints the top event counts to the console and plots them as a bar chart.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset containing at least an \"event\" column.\n",
    "        topn (int): Number of top event categories to display and plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes the DataFrame contains a column named 'event'.\n",
    "        - If df is empty, the function prints a warning and returns without plotting.\n",
    "        - Uses matplotlib (no seaborn, per project guidelines).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Empty DataFrame — build or load the dataset first.\")\n",
    "        return\n",
    "    if 'event' not in df.columns:\n",
    "        print(\"Missing 'event' column — please check your extraction step.\")\n",
    "        return\n",
    "\n",
    "    counts = df['event'].value_counts().head(topn)\n",
    "    print(\"Top event counts:\\n\", counts)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    counts.plot(kind='bar')\n",
    "    plt.title(f\"Top {topn} play-by-play events\")\n",
    "    plt.xlabel(\"Event type\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb2cc7-ee70-4a4a-9271-326c0a4a7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = df_small  \n",
    "\n",
    "quick_event_counts(target_df, topn=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387734d7-e688-4821-a5b7-bccb0216c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-game interactive browser  ---\n",
    "# Browse events per game with season-type toggle and on-rink plotting.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import VBox, HBox, Dropdown, IntSlider, Output\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= 1) Load cleaned dataset (auto-detect .gz / .zip) =========\n",
    "DATA_CANDIDATES = [\n",
    "    \n",
    "    \"data/clean/events_regular_2016-2023.csv.zip\",\n",
    "     \"data/clean/events_regular_2020-2021.csv.zip\",\n",
    "    \n",
    "]\n",
    "\n",
    "def _load_any(paths):\n",
    "    for p in paths:\n",
    "        path = Path(p)\n",
    "        if path.exists():\n",
    "            comp = \"gzip\" if path.suffix == \".gz\" or path.suffixes[-2:] == [\".csv\", \".gz\"] else \\\n",
    "                   \"zip\"  if path.suffix == \".zip\" or path.suffixes[-2:] == [\".csv\", \".zip\"] else None\n",
    "            print(f\"[LOAD] {path} (compression={comp})\")\n",
    "            return pd.read_csv(path, compression=comp)\n",
    "    raise FileNotFoundError(\"No cleaned dataset found. Checked:\\n  - \" + \"\\n  - \".join(paths))\n",
    "\n",
    "path = [\n",
    "    \n",
    "    \"data/clean/events_regular_2016-2023.csv.zip\",\n",
    "    \n",
    "]\n",
    "df0 = _load_any(path)\n",
    "\n",
    "# ========= 2) Sanity checks & minimal enrich =========\n",
    "required_cols = {\n",
    "    \"event\",\"secondaryType\",\"period\",\"periodTime\",\"dateTime\",\"team\",\"x\",\"y\",\"game_id\",\"game_type\"\n",
    "}\n",
    "missing = required_cols - set(df0.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Dataset missing columns {sorted(missing)}. \"\n",
    "                     f\"Please rebuild with the updated build_dataset() that adds game_id and game_type.\")\n",
    "\n",
    "# Derive season_start_year from dateTime (Aug–Jul season logic)\n",
    "def _season_from_dt(dt_str):\n",
    "    try:\n",
    "        y = int(str(dt_str)[:4]); m = int(str(dt_str)[5:7])\n",
    "        return y if m >= 8 else (y - 1)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "if \"season_start_year\" not in df0.columns:\n",
    "    df0[\"season_start_year\"] = df0[\"dateTime\"].astype(str).map(_season_from_dt)\n",
    "\n",
    "# ========= 3) Widgets =========\n",
    "# Season options from derived season_start_year\n",
    "season_vals = sorted([int(s) for s in df0[\"season_start_year\"].dropna().unique().tolist()])\n",
    "season_dd   = Dropdown(options=[\"(all)\"] + season_vals, value=\"(all)\", description=\"Season:\")\n",
    "gtype_dd    = Dropdown(options=[\"regular\",\"playoffs\"], value=\"regular\", description=\"Type:\")\n",
    "game_dd     = Dropdown(options=[], description=\"Game ID:\")\n",
    "event_idx   = IntSlider(value=0, min=0, max=0, step=1, description=\"Event #\")\n",
    "\n",
    "out_plot = Output()\n",
    "out_text = Output()\n",
    "\n",
    "# ========= 4) Helpers =========\n",
    "def _rink(ax):\n",
    "    \"\"\"Minimal rink outline for quick plotting (not to scale).\"\"\"\n",
    "    ax.axhline(0, lw=0.5, alpha=0.3)\n",
    "    ax.set_xlim(-100, 100)\n",
    "    ax.set_ylim(-42, 42)\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_title(\"Rink\")\n",
    "\n",
    "def _refresh_games(*_):\n",
    "    sub = df0.copy()\n",
    "\n",
    "    # filter by game_type\n",
    "    if gtype_dd.value:\n",
    "        sub = sub[sub[\"game_type\"] == gtype_dd.value]\n",
    "\n",
    "    # filter by season_start_year if selected\n",
    "    if season_dd.value != \"(all)\":\n",
    "        sub = sub[sub[\"season_start_year\"] == season_dd.value]\n",
    "\n",
    "    games = sub[\"game_id\"].dropna().unique().tolist()\n",
    "    games.sort()\n",
    "\n",
    "    game_dd.options = games or [\"(none)\"]\n",
    "    game_dd.value = games[0] if games else \"(none)\"\n",
    "    _refresh_event_slider()\n",
    "\n",
    "def _refresh_event_slider(*_):\n",
    "    if game_dd.value in (None, \"(none)\"):\n",
    "        event_idx.max = 0\n",
    "        event_idx.value = 0\n",
    "        _render()\n",
    "        return\n",
    "    sub = df0[(df0[\"game_id\"] == game_dd.value)]\n",
    "    event_idx.max = max(len(sub) - 1, 0)\n",
    "    event_idx.value = 0\n",
    "    _render()\n",
    "\n",
    "def _render(*_):\n",
    "    out_plot.clear_output()\n",
    "    out_text.clear_output()\n",
    "\n",
    "    if game_dd.value in (None, \"(none)\"):\n",
    "        with out_text:\n",
    "            print(\"No game selected.\")\n",
    "        return\n",
    "\n",
    "    sub = df0[df0[\"game_id\"] == game_dd.value].reset_index(drop=True)\n",
    "    if sub.empty:\n",
    "        with out_text:\n",
    "            print(\"No events for this game.\")\n",
    "        return\n",
    "\n",
    "    # clamp index\n",
    "    i = int(event_idx.value)\n",
    "    if i < 0: i = 0\n",
    "    if i >= len(sub): i = len(sub) - 1\n",
    "\n",
    "    row = sub.iloc[i]\n",
    "\n",
    "    with out_text:\n",
    "        # Header info\n",
    "        print(f\"Game: {row['game_id']} | Type: {row.get('game_type','?')} | \"\n",
    "              f\"Season: {int(row['season_start_year']) if pd.notna(row['season_start_year']) else 'n/a'}-\"\n",
    "              f\"{str(int(row['season_start_year'])+1)[-2:] if pd.notna(row['season_start_year']) else 'n/a'}\")\n",
    "        print(f\"Event #{i+1}/{len(sub)}: {row['event']}  | Period {row['period']} @ {row['periodTime']}\")\n",
    "        print(f\"Team: {row['team']}  | dateTime: {row['dateTime']}\")\n",
    "\n",
    "        # No coordinates → print a note (as required)\n",
    "        if pd.isna(row[\"x\"]) or pd.isna(row[\"y\"]):\n",
    "            print(\"(No coordinates for this event)\")\n",
    "\n",
    "    with out_plot:\n",
    "        fig, ax = plt.subplots(figsize=(6,5))\n",
    "        _rink(ax)\n",
    "        # Draw the point if coordinates exist\n",
    "        if pd.notna(row[\"x\"]) and pd.notna(row[\"y\"]):\n",
    "            ax.scatter([row[\"x\"]], [row[\"y\"]], s=70, alpha=0.9)\n",
    "            ax.set_title(f\"{row['event']} ({row['team']})  P{int(row['period'])} {row['periodTime']}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ========= 5) Wiring & initial display =========\n",
    "season_dd.observe(_refresh_games, \"value\")\n",
    "gtype_dd.observe(_refresh_games, \"value\")\n",
    "game_dd.observe(_render, \"value\")\n",
    "event_idx.observe(_render, \"value\")\n",
    "\n",
    "_refresh_games()  # populate game list & render\n",
    "\n",
    "ui_game = VBox([\n",
    "    HBox([season_dd, gtype_dd]),\n",
    "    HBox([game_dd, event_idx]),\n",
    "    out_text,\n",
    "    out_plot\n",
    "])\n",
    "\n",
    "display(ui_game)\n",
    "\n",
    "# ========= 6)  Export this widget as a standalone HTML to embed in blog =========\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "from pathlib import Path\n",
    "Path(\"_includes\").mkdir(exist_ok=True)\n",
    "embed_minimal_html(\"_includes/interactive_widget.html\", views=[ui_game], title=\"Explorateur LNH\")\n",
    "print(\"Exported -> _includes/interactive_widget.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35d361-d4f7-4769-8dcd-f3d044132afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "\n",
    "Path(\"_includes\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "embed_minimal_html(\"_includes/interactive_widget.html\", views=[ui_game], title=\"Explorateur LNH\")\n",
    "\n",
    "print(\"OK → _includes/interactive_widget.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c1d7c-1720-40c3-b5b5-da3ed62c403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_shots_and_goals(json_obj: dict, game_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Version corrigée pour le nouveau format d'API NHL\n",
    "    Utilise eventOwnerTeamId au lieu de team\n",
    "    \"\"\"\n",
    "    plays = json_obj.get(\"plays\", [])\n",
    "    rows = []\n",
    "    \n",
    "    # Récupérer les équipes home/away pour mapper les IDs\n",
    "    home_team = json_obj.get(\"homeTeam\", {}).get(\"abbrev\")\n",
    "    away_team = json_obj.get(\"awayTeam\", {}).get(\"abbrev\")\n",
    "    home_team_id = json_obj.get(\"homeTeam\", {}).get(\"id\")\n",
    "    away_team_id = json_obj.get(\"awayTeam\", {}).get(\"id\")\n",
    "    \n",
    "    # Mapping des IDs d'équipe vers les abbreviations\n",
    "    team_id_map = {}\n",
    "    if home_team_id and home_team:\n",
    "        team_id_map[home_team_id] = home_team\n",
    "    if away_team_id and away_team:\n",
    "        team_id_map[away_team_id] = away_team\n",
    "    \n",
    "    for p in plays:\n",
    "        event = p.get(\"typeDescKey\")\n",
    "        if event not in (\"shot-on-goal\", \"goal\"):\n",
    "            continue\n",
    "\n",
    "        details = p.get(\"details\", {}) or {}\n",
    "        periodD = p.get(\"periodDescriptor\", {}) or {}\n",
    "\n",
    "        # EXTRACTION DE L'ÉQUIPE via eventOwnerTeamId\n",
    "        team_id = details.get(\"eventOwnerTeamId\")\n",
    "        team_name = team_id_map.get(team_id) if team_id else None\n",
    "        \n",
    "        # Si pas dans le mapping, utiliser l'ID directement\n",
    "        if team_name is None and team_id:\n",
    "            team_name = f\"TEAM_{team_id}\"\n",
    "\n",
    "        # Extraction des coordonnées\n",
    "        x_coord = details.get(\"xCoord\")\n",
    "        y_coord = details.get(\"yCoord\")\n",
    "        \n",
    "        # Ignorer si pas de coordonnées\n",
    "        if x_coord is None or y_coord is None:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"game_id\": game_id,\n",
    "            \"period\": periodD.get(\"number\"),\n",
    "            \"periodTime\": p.get(\"timeInPeriod\"),\n",
    "            \"team\": team_name,\n",
    "            \"eventType\": \"GOAL\" if event == \"goal\" else \"SHOT\",\n",
    "            \"shooter\": details.get(\"shooterPlayerId\"),\n",
    "            \"goalie\": details.get(\"goalieInNetId\"),\n",
    "            \"x\": x_coord,\n",
    "            \"y\": y_coord,\n",
    "            \"shotType\": details.get(\"shotType\"),\n",
    "            \"emptyNet\": details.get(\"emptyNet\", False),\n",
    "            \"strength\": details.get(\"strength\"),\n",
    "            \"dateTime\": p.get(\"timeInPeriod\"),\n",
    "            \"eventOwnerTeamId\": team_id  # Garder l'ID original aussi\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d1621-eca4-40c7-9eaa-8dbaa7cbfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_clean_shots_dataset(start=2016, end=2024, gtype=\"regular\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all shots and goals from cached JSONs into one clean DataFrame.\n",
    "    \"\"\"\n",
    "    root = CACHE_ROOT\n",
    "    tcode = GAME_TYPE_MAP[gtype]\n",
    "    dfs = []\n",
    "\n",
    "    for year in range(start, end+1):\n",
    "        d = root / str(year) / f\"type-{tcode}\"\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for f in tqdm(list(d.glob(\"*.json\")), desc=f\"{year} {gtype}\"):\n",
    "            try:\n",
    "                js = json.loads(f.read_text())\n",
    "                game_id = f.stem\n",
    "                df = extract_shots_and_goals(js, game_id)\n",
    "                if not df.empty:\n",
    "                    df[\"season_start_year\"] = year\n",
    "                    df[\"game_type\"] = gtype\n",
    "                    dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(\"[WARN]\", f, e)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "df_clean = build_clean_shots_dataset(2016, 2024, \"regular\")\n",
    "df_clean.to_csv(\"data/clean/shots_goals_regular_2016_2024.csv.zip\", index=False, compression=\"zip\")\n",
    "print(df_clean.head(10))\n",
    "print(\"Fichier sauvegardé : data/clean/shots_goals_regular_2016_2024.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b218748-1b44-438d-abb5-6ec0f512c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build cleaned shots/goals CSVs from cached JSONs for selected seasons ---\n",
    "# Input  : data/raw/<YEAR>/type-02/*.json (regular), data/raw/<YEAR>/type-03/*.json (playoffs)\n",
    "# Output : data/clean/shots_goals_<type>_<YYYY[_YYYY]>.csv.gz  (SHOT & GOAL rows only)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Config: adjust YEARS and TYPES to what you need\n",
    "# -------------------------------------------------------------------\n",
    "YEARS: List[int] = [2018, 2019, 2020]                 # e.g. [2020] for season 2020-21 only; or [2018, 2019, 2020]\n",
    "TYPES: Tuple[str, ...] = (\"regular\", \"playoffs\")     # (\"regular\",) or (\"playoffs\",) or (\"regular\",\"playoffs\")\n",
    "\n",
    "CACHE_ROOT = Path(\"data/raw\")\n",
    "CLEAN_ROOT = Path(\"data/clean\")\n",
    "CLEAN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TYPE2CODE: Dict[str, str] = {\"preseason\":\"01\",\"regular\":\"02\",\"playoffs\":\"03\",\"allstar\":\"04\"}\n",
    "CODE2TYPE: Dict[str, str] = {v:k for k,v in TYPE2CODE.items()}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Core extractors\n",
    "# -------------------------------------------------------------------\n",
    "def extract_shots_and_goals(json_obj: dict, game_id: str, game_type_label: str, season_start_year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert ONE game's play-by-play JSON (new gamecenter) into a tidy table\n",
    "    containing ONLY 'shot-on-goal' and 'goal' events.\n",
    "\n",
    "    Returned columns include (minimum per assignment):\n",
    "      - game_id, season_start_year, game_type\n",
    "      - period, periodTime\n",
    "      - team (shooting team)\n",
    "      - eventType: 'SHOT' | 'GOAL'\n",
    "      - x, y (ice coordinates; may be NaN)\n",
    "      - shooter, goalie\n",
    "      - shotType\n",
    "      - emptyNet (bool)\n",
    "      - strength (EV/PP/SH etc., often present for GOAL)\n",
    "      - dateTime (UTC if available)\n",
    "    \"\"\"\n",
    "    plays = json_obj.get(\"plays\", []) or []\n",
    "    rows = []\n",
    "    for p in plays:\n",
    "        evt = (p.get(\"typeDescKey\") or p.get(\"typeCode\") or \"\").lower()\n",
    "        if evt not in (\"shot-on-goal\", \"goal\"):\n",
    "            continue\n",
    "\n",
    "        det   = p.get(\"details\", {}) or {}\n",
    "        team  = p.get(\"team\", {}) or {}\n",
    "        perD  = p.get(\"periodDescriptor\", {}) or {}\n",
    "\n",
    "        rows.append({\n",
    "            \"game_id\":            game_id,\n",
    "            \"season_start_year\":  season_start_year,\n",
    "            \"game_type\":          game_type_label,     # 'regular' or 'playoffs'\n",
    "            \"period\":             perD.get(\"number\"),\n",
    "            \"periodTime\":         p.get(\"timeInPeriod\") or det.get(\"timeInPeriod\"),\n",
    "            \"team\":               team.get(\"triCode\") or team.get(\"name\"),\n",
    "            \"eventType\":          \"GOAL\" if evt == \"goal\" else \"SHOT\",\n",
    "            \"x\":                  det.get(\"xCoord\"),\n",
    "            \"y\":                  det.get(\"yCoord\"),\n",
    "            \"shooter\":            det.get(\"shooterName\"),\n",
    "            \"goalie\":             det.get(\"goalieName\"),\n",
    "            \"shotType\":           det.get(\"shotType\"),\n",
    "            \"emptyNet\":           bool(det.get(\"emptyNet\", False)),\n",
    "            \"strength\":           det.get(\"strength\"),\n",
    "            \"dateTime\":           p.get(\"timeUTC\") or det.get(\"eventOwnerTeamTime\"),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _cache_dir_for(year: int, game_type: str) -> Path:\n",
    "    \"\"\"Return the cache directory where raw JSON files were saved for a given season & type.\"\"\"\n",
    "    tcode = TYPE2CODE[game_type]\n",
    "    return CACHE_ROOT / str(year) / f\"type-{tcode}\"\n",
    "\n",
    "def _iter_game_files(years: Iterable[int], game_type: str) -> List[Path]:\n",
    "    \"\"\"\n",
    "    List all cached JSON files for given seasons and game type.\n",
    "    Only scans local cache; does not fetch from the web.\n",
    "    \"\"\"\n",
    "    files: List[Path] = []\n",
    "    for y in years:\n",
    "        d = _cache_dir_for(y, game_type)\n",
    "        if d.exists():\n",
    "            files.extend(sorted(d.glob(\"*.json\")))\n",
    "    return files\n",
    "\n",
    "def build_clean_for(years: Iterable[int], game_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a cleaned DataFrame (shots & goals only) for the selected seasons and one game type.\n",
    "    Reads ONLY from local cache under data/raw.\n",
    "    \"\"\"\n",
    "    files = _iter_game_files(years, game_type)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No cached JSON found for {list(years)} {game_type} under {CACHE_ROOT}/<year>/type-*/\")\n",
    "\n",
    "    dfs = []\n",
    "    for f in tqdm(files, desc=f\"Extract {game_type} ({min(years)}..{max(years)})\"):\n",
    "        try:\n",
    "            js = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "            game_id = f.stem\n",
    "            year = int(f.parent.parent.name)   # the 'year' folder name\n",
    "            df = extract_shots_and_goals(js, game_id, game_type, year)\n",
    "            if not df.empty:\n",
    "                dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN]\", f, e)\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "def _season_suffix_range(years: Iterable[int]) -> str:\n",
    "    \"\"\"\n",
    "    Build a short suffix for filenames, e.g. [2020] -> '2020_2021', [2018,2019,2020] -> '2018_2020'.\n",
    "    For a single start-year, we reflect the hockey span (e.g., 2020 -> 2020_2021).\n",
    "    \"\"\"\n",
    "    years = sorted(set(years))\n",
    "    if len(years) == 1:\n",
    "        y = years[0]\n",
    "        return f\"{y}_{y+1}\"\n",
    "    return f\"{years[0]}_{years[-1]}\"\n",
    "\n",
    "def save_clean_df(df: pd.DataFrame, out_path: Path) -> Path:\n",
    "    \"\"\"Save DataFrame as CSV or CSV.GZ based on suffix.\"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.suffix == \".gz\" or out_path.suffixes[-2:] == [\".csv\", \".zip\"]:\n",
    "        df.to_csv(out_path, index=False, compression=\"zip\")\n",
    "    else:\n",
    "        df.to_csv(out_path, index=False)\n",
    "    print(f\"[OK] Saved {out_path}  rows={len(df)}\")\n",
    "    return out_path\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Run: build per requested type, then (optionally) a merged file\n",
    "# -------------------------------------------------------------------\n",
    "saved_paths: List[Path] = []\n",
    "\n",
    "for gtype in TYPES:\n",
    "    df_clean = build_clean_for(YEARS, gtype)\n",
    "    if df_clean.empty:\n",
    "        print(f\"[WARN] Empty result for {YEARS} {gtype}\")\n",
    "        continue\n",
    "    suffix = _season_suffix_range(YEARS)\n",
    "    outp = CLEAN_ROOT / f\"shots_goals_{gtype}_{suffix}.csv.zip\"\n",
    "    save_clean_df(df_clean, outp)\n",
    "    saved_paths.append(outp)\n",
    "\n",
    "# Optional: merge all requested types (if more than one) into one file\n",
    "if len(saved_paths) >= 2:\n",
    "    merged = pd.concat([pd.read_csv(p, compression=\"zip\") for p in saved_paths], ignore_index=True)\n",
    "    outm = CLEAN_ROOT / f\"shots_goals_{_season_suffix_range(YEARS)}.csv.zip\"\n",
    "    save_clean_df(merged, outm)\n",
    "    saved_paths.append(outm)\n",
    "\n",
    "print(\"\\n[SUMMARY]\")\n",
    "for p in saved_paths:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044ee4c-5616-4090-9a44-14ac6220804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Simple visualizations for IFT6758 Étape 1 (Q4) ---\n",
    "# This cell produces:\n",
    "#   - Q1: Bar chart of total shots vs goals by shot type (single season)\n",
    "#   - Q1a: Goal probability vs distance curves for seasons 2018-19 to 2020-21\n",
    "#   - Q2: Heatmap of goal% by distance bin and shot type (single season)\n",
    "#\n",
    "# Requirements: pandas, numpy, matplotlib (no seaborn needed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "# ---------------------------\n",
    "# Config: input files & season\n",
    "# ---------------------------\n",
    "# Point this to your cleaned shots/goals dataset generated in Step 3.\n",
    "# Accepts .csv, .csv.gz or .csv.zip\n",
    "CLEAN_PATHS = [\n",
    "    \"data/clean/shots_goals_regular_2018_2020.csv.zip\",\n",
    "    \"data/clean/shots_goals_playoffs_2018_2020.csv.zip\",\n",
    "    \"data/clean/shots_goals_2018_2020.csv.zip\",\n",
    "   \n",
    "]\n",
    "\n",
    "SEASON_FOR_SINGLE = 2020  # 2020-21 season (season_start_year=2020)\n",
    "SEASONS_FOR_CURVES = [2018, 2019, 2020]  # 2018-19, 2019-20, 2020-21\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def load_first_available(paths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Try to load the first existing cleaned dataset among candidate paths.\n",
    "    Supports .gz and .zip via pandas' 'compression' parameter.\n",
    "    \"\"\"\n",
    "    for p in paths:\n",
    "        path = Path(p)\n",
    "        if path.exists():\n",
    "            comp = None\n",
    "            if path.suffix == \".gz\" or path.suffixes[-2:] == [\".csv\", \".gz\"]:\n",
    "                comp = \"gzip\"\n",
    "            elif path.suffix == \".zip\" or path.suffixes[-2:] == [\".csv\", \".zip\"]:\n",
    "                comp = \"zip\"\n",
    "            print(f\"[LOAD] {path} (compression={comp})\")\n",
    "            return pd.read_csv(path, compression=comp)\n",
    "    raise FileNotFoundError(\n",
    "        \"No cleaned dataset found. Please generate your cleaned shots/goals file first.\\n\"\n",
    "        \"Checked:\\n  - \" + \"\\n  - \".join(paths)\n",
    "    )\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize column names and types expected by this visualization cell.\n",
    "    Expected semantics:\n",
    "      - secondaryType: shot type name (if missing, fallback to 'shotType')\n",
    "      - eventType: 'SHOT' or 'GOAL' (if missing, derive from 'event')\n",
    "      - season_start_year: infer from dateTime if missing\n",
    "      - x, y present (may contain NaN)\n",
    "    \"\"\"\n",
    "   \n",
    "    if \"secondaryType\" not in df.columns and \"shotType\" in df.columns:\n",
    "        df = df.assign(secondaryType=df[\"shotType\"])\n",
    "    if \"secondaryType\" not in df.columns:\n",
    "        raise ValueError(\"Missing required column 'secondaryType' (no 'shotType' to fallback).\")\n",
    "\n",
    "    # --- eventType fallback from event ---\n",
    "    if \"eventType\" not in df.columns:\n",
    "        if \"event\" in df.columns:\n",
    "            mapped = df[\"event\"].astype(str).str.lower().map({\n",
    "                \"goal\": \"GOAL\",\n",
    "                \"shot-on-goal\": \"SHOT\",\n",
    "                \"shot\": \"SHOT\",\n",
    "            })\n",
    "            df = df.assign(eventType=mapped)\n",
    "        else:\n",
    "            raise ValueError(\"Neither 'eventType' nor 'event' found to identify SHOT/GOAL.\")\n",
    "\n",
    "    # --- coordinates presence check (allow NaN) ---\n",
    "    for c in (\"x\", \"y\"):\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing required column '{c}'.\")\n",
    "\n",
    "    # --- season inference if absent ---\n",
    "    if \"season_start_year\" not in df.columns:\n",
    "        if \"dateTime\" not in df.columns:\n",
    "            raise ValueError(\"Missing 'season_start_year' and 'dateTime' to infer it.\")\n",
    "        def _season_from_dt(dt_str):\n",
    "            try:\n",
    "                y = int(str(dt_str)[:4]); m = int(str(dt_str)[5:7])\n",
    "                return y if m >= 8 else (y - 1)\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        df[\"season_start_year\"] = df[\"dateTime\"].astype(str).map(_season_from_dt)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_distance_xy(x: pd.Series, y: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute shot distance to the attacking net using NHL rink coordinates.\n",
    "    We assume the offensive net is near x=89, y=0. Distances are in 'rink feet' units.\n",
    "    \"\"\"\n",
    "    return np.sqrt((89 - x.astype(float))**2 + (0 - y.astype(float))**2)\n",
    "\n",
    "def season_filter(df: pd.DataFrame, season_start_year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return rows for a specific NHL season (start-year), e.g., 2020 for 2020-21.\n",
    "    \"\"\"\n",
    "    return df[df[\"season_start_year\"] == season_start_year].copy()\n",
    "\n",
    "def summarize_shots_goals_by_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group by 'secondaryType' to compute total shots, total goals and goal rate.\n",
    "    Only 'SHOT' and 'GOAL' rows are considered.\n",
    "    \"\"\"\n",
    "    sub = df[df[\"eventType\"].isin([\"SHOT\", \"GOAL\"])].copy()\n",
    "    # count by (type, eventType)\n",
    "    ct = sub.groupby([\"secondaryType\", \"eventType\"]).size().unstack(fill_value=0)\n",
    "    for col in (\"SHOT\", \"GOAL\"):\n",
    "        if col not in ct.columns:\n",
    "            ct[col] = 0\n",
    "    ct[\"goal_rate\"] = ct[\"GOAL\"] / (ct[\"GOAL\"] + ct[\"SHOT\"]).replace({0: np.nan})\n",
    "    return ct.sort_values(by=[\"SHOT\", \"GOAL\"], ascending=False)\n",
    "\n",
    "def binned_goal_rate(df: pd.DataFrame, bin_width: int = 2, max_dist: int = 90) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute goal rate over distance bins for the given dataframe (SHOT & GOAL rows).\n",
    "    Returns (bin_centers, goal_rate_array).\n",
    "    \"\"\"\n",
    "    sub = df[df[\"eventType\"].isin([\"SHOT\",\"GOAL\"])].copy()\n",
    "    sub[\"distance\"] = compute_distance_xy(sub[\"x\"], sub[\"y\"])\n",
    "    bins = np.arange(0, max_dist + bin_width, bin_width)\n",
    "    labels = bins[:-1] + bin_width / 2.0\n",
    "    sub[\"d_bin\"] = pd.cut(sub[\"distance\"], bins=bins, include_lowest=True, labels=labels)\n",
    "    grp = sub.groupby(\"d_bin\")[\"eventType\"].value_counts().unstack(fill_value=0)\n",
    "    shots = grp.get(\"SHOT\", pd.Series(0, index=grp.index))\n",
    "    goals = grp.get(\"GOAL\", pd.Series(0, index=grp.index))\n",
    "    rate = (goals / (shots + goals).replace({0: np.nan})).astype(float)\n",
    "    x_centers = grp.index.astype(float)  # bin centers\n",
    "    return x_centers.values, rate.values\n",
    "\n",
    "def heatmap_goal_rate_by_distance_and_type(df: pd.DataFrame, bin_width: int = 5, max_dist: int = 90) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build a 2D matrix of goal% indexed by distance bins (rows) and shot types (columns).\n",
    "    Returns (matrix, col_labels) where matrix shape is [n_bins, n_types].\n",
    "    \"\"\"\n",
    "    sub = df[df[\"eventType\"].isin([\"SHOT\",\"GOAL\"])].copy()\n",
    "    sub[\"distance\"] = compute_distance_xy(sub[\"x\"], sub[\"y\"])\n",
    "    # distance bins\n",
    "    bins = np.arange(0, max_dist + bin_width, bin_width)\n",
    "    sub[\"d_bin\"] = pd.cut(sub[\"distance\"], bins=bins, include_lowest=True)\n",
    "    # pivot: within each (d_bin, type), compute goals / total\n",
    "    def _rate(s):\n",
    "        s = s.dropna()\n",
    "        if s.empty: return np.nan\n",
    "        goals = (s == \"GOAL\").sum()\n",
    "        total = s.size\n",
    "        return goals / total if total > 0 else np.nan\n",
    "\n",
    "    pivot = sub.pivot_table(index=\"d_bin\", columns=\"secondaryType\", values=\"eventType\", aggfunc=_rate)\n",
    "    # Sort columns alphabetically for stable display\n",
    "    pivot = pivot.reindex(sorted(pivot.columns), axis=1)\n",
    "    return pivot.values, pivot.columns.tolist(), [str(idx) for idx in pivot.index]\n",
    "\n",
    "# ---------------------------\n",
    "# Load & normalize\n",
    "# ---------------------------\n",
    "df_all = load_first_available(CLEAN_PATHS)\n",
    "\n",
    "\n",
    "if \"secondaryType\" not in df_all.columns and \"shotType\" in df_all.columns:\n",
    "    df_all[\"secondaryType\"] = df_all[\"shotType\"]\n",
    "\n",
    "df_all = ensure_columns(df_all)\n",
    "\n",
    "# ---------------------------\n",
    "# Q1: Shots vs Goals by shot type (single season)\n",
    "# ---------------------------\n",
    "df_one = season_filter(df_all, SEASON_FOR_SINGLE)\n",
    "by_type = summarize_shots_goals_by_type(df_one)\n",
    "\n",
    "print(f\"[INFO] Season {SEASON_FOR_SINGLE}-{str(SEASON_FOR_SINGLE+1)[-2:]} rows:\", len(df_one))\n",
    "display(by_type.head(10))\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Create assets directory if it doesn't exist\n",
    "Path(\"assets\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"Folder ready:\", Path('assets').resolve())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "by_type[[\"SHOT\",\"GOAL\"]].plot(kind=\"bar\", ax=plt.gca(), width=0.8)\n",
    "plt.title(f\"Shots vs Goals by Shot Type — Season {SEASON_FOR_SINGLE}-{str(SEASON_FOR_SINGLE+1)[-2:]}\")\n",
    "plt.xlabel(\"Shot type (secondaryType)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.savefig(\"Shots vs Goals by Shot Type.png\", dpi=150)\n",
    "\n",
    "\n",
    "# Optionally print quick takeaways:\n",
    "most_common = by_type[\"SHOT\"].idxmax() if not by_type.empty else \"n/a\"\n",
    "most_dangerous = by_type[\"goal_rate\"].idxmax() if \"goal_rate\" in by_type.columns and by_type[\"goal_rate\"].notna().any() else \"n/a\"\n",
    "print(f\"[TAKEAWAY] Most common shot type: {most_common}\")\n",
    "print(f\"[TAKEAWAY] Highest goal-rate shot type: {most_dangerous}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Q1a: Goal probability vs distance for 2018-19, 2019-20, 2020-21\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(9,5))\n",
    "for sy in SEASONS_FOR_CURVES:\n",
    "    df_s = season_filter(df_all, sy)\n",
    "    if df_s.empty:\n",
    "        print(f\"[WARN] No data for season_start_year={sy}\")\n",
    "        continue\n",
    "    x_mid, y_rate = binned_goal_rate(df_s, bin_width=2, max_dist=90)\n",
    "    plt.plot(x_mid, y_rate, marker=\"o\", linestyle=\"-\", label=f\"{sy}-{str(sy+1)[-2:]}\")\n",
    "plt.title(\"Goal probability vs distance (shots+goals)\\nSeasons 2018-19 to 2020-21\")\n",
    "plt.xlabel(\"Distance to net (feet)\"); plt.ylabel(\"Goal rate\")\n",
    "plt.ylim(0, 1.0); plt.legend(); plt.tight_layout(); plt.show()\n",
    "plt.savefig(\"assets/Goal probability vs distance.png\", dpi=150)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Q2: Heatmap of goal% by distance × shot type (single season)\n",
    "# ---------------------------\n",
    "mat, col_labels, row_labels = heatmap_goal_rate_by_distance_and_type(df_one, bin_width=5, max_dist=90)\n",
    "\n",
    "plt.figure(figsize=(11,6))\n",
    "im = plt.imshow(mat, aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04, label=\"Goal rate\")\n",
    "plt.xticks(ticks=np.arange(len(col_labels)), labels=col_labels, rotation=45, ha=\"right\")\n",
    "plt.yticks(ticks=np.arange(len(row_labels)), labels=row_labels)\n",
    "plt.title(f\"Goal% by distance bin and shot type — Season {SEASON_FOR_SINGLE}-{str(SEASON_FOR_SINGLE+1)[-2:]}\")\n",
    "plt.xlabel(\"Shot type\"); plt.ylabel(\"Distance bins (feet)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.savefig(\"assets/Goals by distance.png\", dpi=150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a32905-9d33-4641-a3e7-c4fe4e20a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cellule : Calcul de la densité de tir ---\n",
    "# Cette fonction calcule où les équipes tirent le plus sur la patinoire\n",
    "# Utilise la méthode KDE (estimation de densité par noyau) pour lisser les positions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def calculer_densite_tirs(df, annee_saison, equipe=None):\n",
    "    # Filtre par saison\n",
    "    df_saison = df[df['season_start_year'] == annee_saison].copy()\n",
    "    \n",
    "     # Si on veut une équipe en particulier\n",
    "    if equipe:\n",
    "        df_saison = df_saison[df_saison['team'] == equipe]\n",
    "    \n",
    "    # Prendre seulement les tirs et buts\n",
    "    df_tirs = df_saison[df_saison['eventType'].isin(['SHOT', 'GOAL'])]\n",
    "    \n",
    "    # Enlever les données sans coordonnées\n",
    "    df_tirs = df_tirs.dropna(subset=['x', 'y'])\n",
    "    \n",
    "    # Garder seulement les tirs en zone offensive (côté droit)\n",
    "    tirs_offensifs = df_tirs[df_tirs['x'] > 0]\n",
    "\n",
    "    \n",
    "     # S'il y'a pas de données, on arrête tout\n",
    "    if len(tirs_offensifs) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Calcul de la densité avec méthode KDE\n",
    "    coordonnees = tirs_offensifs[['x', 'y']].values.T\n",
    "       # Calcul de la densité avec méthode KDE\n",
    "    coordonnees = tirs_offensifs[['x', 'y']].values.T\n",
    "    try:\n",
    "        kde = gaussian_kde(coordonnees)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur KDE pour {equipe or 'ligue'} ({len(coordonnees[0])} tirs) → {e}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    # Créer une grille pour évaluer la densité\n",
    "    x_grille = np.linspace(0, 100, 50)\n",
    "    y_grille = np.linspace(-42, 42, 50)\n",
    "    X, Y = np.meshgrid(x_grille, y_grille)\n",
    "    coord_grille = np.vstack([X.ravel(), Y.ravel()])\n",
    "    \n",
    "    # Calculer la densité sur toute la grille\n",
    "    densite = kde(coord_grille).reshape(X.shape)\n",
    "    \n",
    "    return {\n",
    "        'x_grille': x_grille,\n",
    "        'y_grille': y_grille, \n",
    "        'densite': densite,\n",
    "        'equipe': equipe,\n",
    "        'saison': annee_saison,\n",
    "        'total_tirs': len(tirs_offensifs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f91b9c-800f-492c-a048-2fe8ce670f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cellule : Graphique interactif avec menu équipes ---\n",
    "# Cette fonction crée la carte avec menu déroulant\n",
    "\n",
    "def creer_carte_interactive(donnees_calcul, liste_equipes):\n",
    "    \"\"\"\n",
    "    Crée une carte interactive avec menu pour choisir l'équipe\n",
    "    donnees_calcul: dictionnaire avec les données de densité\n",
    "    liste_equipes: liste de toutes les équipes disponibles\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # D'abord, je trace la moyenne de toute la ligue\n",
    "    fig.add_trace(go.Contour(\n",
    "        x=donnees_calcul['moyenne_ligue']['x_grille'],\n",
    "        y=donnees_calcul['moyenne_ligue']['y_grille'], \n",
    "        z=donnees_calcul['moyenne_ligue']['densite'],\n",
    "        colorscale='Blues',\n",
    "        showscale=False,\n",
    "        name='Moyenne Ligue',\n",
    "        visible=True  # Visible par défaut\n",
    "    ))\n",
    "    \n",
    "    # Ensuite, je prépare le tracé pour chaque équipe\n",
    "    for i, equipe in enumerate(liste_equipes):\n",
    "        if equipe in donnees_calcul:\n",
    "            fig.add_trace(go.Contour(\n",
    "                x=donnees_calcul[equipe]['x_grille'],\n",
    "                y=donnees_calcul[equipe]['y_grille'],\n",
    "                z=donnees_calcul[equipe]['densite'],\n",
    "                colorscale='Reds',\n",
    "                showscale=False,\n",
    "                name=equipe,\n",
    "                visible=False  # Cachées au début\n",
    "            ))\n",
    "    \n",
    "    # Maintenant, je crée le menu déroulant\n",
    "    boutons_menu = []\n",
    "    \n",
    "    # Option 1: Voir la moyenne de la ligue\n",
    "    boutons_menu.append(dict(\n",
    "        label='Moyenne Ligue',\n",
    "        method='update',\n",
    "        args=[{'visible': [True] + [False] * len(liste_equipes)}]\n",
    "    ))\n",
    "    \n",
    "    # Options 2...: Voir chaque équipe\n",
    "    for i, equipe in enumerate(liste_equipes):\n",
    "        if equipe in donnees_calcul:\n",
    "            # Je prépare quel graphique montrer/cacher\n",
    "            visibilite = [False] * (len(liste_equipes) + 1)\n",
    "            visibilite[0] = False    # Cache la moyenne ligue\n",
    "            visibilite[i + 1] = True  # Montre cette équipe\n",
    "            \n",
    "            boutons_menu.append(dict(\n",
    "                label=equipe,\n",
    "                method='update',\n",
    "                args=[{'visible': visibilite}]\n",
    "            ))\n",
    "    \n",
    "    # Finalement, je configure l'apparence\n",
    "    fig.update_layout(\n",
    "        title=f\"Carte des Tirs Offensifs - Saison {donnees_calcul['moyenne_ligue']['saison']}\",\n",
    "        xaxis_title='Distance du filet (pieds)',\n",
    "        yaxis_title='Position latérale (pieds)',\n",
    "        updatemenus=[dict(\n",
    "            type=\"dropdown\",\n",
    "            direction=\"down\",\n",
    "            x=0.1,\n",
    "            y=1.15,\n",
    "            buttons=boutons_menu\n",
    "        )]\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9134bd-fb7e-4f4f-af48-550261fc42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Créer un dossier pour les graphiques dans data/\n",
    "output_dir = \"data/graphiques\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Dossier créé : {output_dir}\")\n",
    "\n",
    "\n",
    "# generer graphiques \n",
    "for annee_saison in saisons:\n",
    "    print(f\"Création graphique saison {annee_saison}-{annee_saison+1}...\")\n",
    "    \n",
    "    donnees_calcul = {}\n",
    "    \n",
    "    # Moyenne ligue pour cette saison\n",
    "    moyenne_ligue = calculer_densite_tirs(df_normalise, annee_saison=annee_saison)\n",
    "    if moyenne_ligue is None:\n",
    "        print(f\"Aucune donnée moyenne ligue pour {annee_saison}\")\n",
    "        continue\n",
    "    else:\n",
    "        donnees_calcul['moyenne_ligue'] = moyenne_ligue\n",
    "        print(f\"Moyenne ligue: {moyenne_ligue['total_tirs']} tirs\")\n",
    "    \n",
    "    # Liste des équipes pour cette saison\n",
    "    liste_equipes_saison = sorted([\n",
    "        team for team in df_normalise[df_normalise['season_start_year'] == annee_saison]['team'].unique() \n",
    "        if team and pd.notna(team) and team != \"UNK\"\n",
    "    ])\n",
    "    \n",
    "    print(f\"Équipes disponibles: {len(liste_equipes_saison)}\")\n",
    "    \n",
    "    # Densité par équipe (5 équipes pour test)\n",
    "    equipes_test = liste_equipes_saison[:5]\n",
    "    equipes_avec_donnees = 0\n",
    "    \n",
    "    for equipe in equipes_test:\n",
    "        dens = calculer_densite_tirs(df_normalise, annee_saison=annee_saison, equipe=equipe)\n",
    "        if dens:\n",
    "            donnees_calcul[equipe] = dens\n",
    "            equipes_avec_donnees += 1\n",
    "    \n",
    "    print(f\"Équipes avec données: {equipes_avec_donnees}/{len(equipes_test)}\")\n",
    "    \n",
    "    # Création du graphique\n",
    "    if len(donnees_calcul) > 1:\n",
    "        fig = creer_carte_interactive(donnees_calcul, equipes_test)\n",
    "        \n",
    "        # Sauvegarde dans le bon dossier\n",
    "        nom_fichier = os.path.join(output_dir, f\"plan_tir_{annee_saison}_{annee_saison+1}.html\")\n",
    "        fig.write_html(nom_fichier)\n",
    "        print(f\"Sauvegardé: {nom_fichier}\")\n",
    "        \n",
    "        # Affichage du dernier graphique\n",
    "        if annee_saison == 2020:\n",
    "            print(\"Affichage du dernier graphique...\")\n",
    "            fig.show()\n",
    "    else:\n",
    "        print(f\"Pas assez de données pour {annee_saison}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7853b-791b-41ed-ae72-bc46bd9f2e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
