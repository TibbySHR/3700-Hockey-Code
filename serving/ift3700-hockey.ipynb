{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e15f5890-f7bf-4578-a105-13019ceb80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: setup & utilities ---\n",
    "import os, json, time, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "CACHE_ROOT = Path(\"../data/raw\")\n",
    "CACHE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"ift6758-course/1.0\"}\n",
    "TIMEOUT = 15\n",
    "THROTTLE_SEC = 0.10\n",
    "MAX_WORKERS = 8\n",
    "MAX_RETRIES = 3\n",
    "BACKOFF_BASE = 0.5\n",
    "\n",
    "GAME_TYPE_MAP = {\n",
    "    \"preseason\": \"01\",\n",
    "    \"regular\": \"02\",\n",
    "    \"playoffs\": \"03\",\n",
    "    \"allstar\": \"04\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_game_id(season_start_year: int, game_type: str, game_number: int) -> str:\n",
    "    \"\"\"\n",
    "    Build a unique NHL GAME_ID string.\n",
    "\n",
    "    Format:\n",
    "        GAME_ID = YYYYTTNNNN\n",
    "            YYYY : season start year (e.g., 2016 for the 2016–17 season)\n",
    "            TT   : game type code\n",
    "                   \"01\" = preseason\n",
    "                   \"02\" = regular season\n",
    "                   \"03\" = playoffs\n",
    "                   \"04\" = all-star\n",
    "            NNNN : four-digit sequential game number\n",
    "\n",
    "    Args:\n",
    "        season_start_year (int): The starting year of the season.\n",
    "        game_type (str): One of {'preseason', 'regular', 'playoffs', 'allstar'}.\n",
    "        game_number (int): Sequential number of the game.\n",
    "\n",
    "    Returns:\n",
    "        str: The constructed GAME_ID.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `game_type` is not one of the supported keys.\n",
    "    \"\"\"\n",
    "    if game_type not in GAME_TYPE_MAP:\n",
    "        raise ValueError(f\"Unknown game_type '{game_type}'\")\n",
    "    return f\"{season_start_year}{GAME_TYPE_MAP[game_type]}{game_number:04d}\"\n",
    "\n",
    "\n",
    "def cache_path(cache_root: Path, game_id: str) -> Path:\n",
    "    \"\"\"\n",
    "    Build the local cache file path for a given GAME_ID.\n",
    "\n",
    "    Args:\n",
    "        cache_root (Path): Root directory of the cache.\n",
    "        game_id (str): The NHL GAME_ID.\n",
    "\n",
    "    Returns:\n",
    "        Path: Full path to the cached JSON file,\n",
    "              e.g. data/raw/2016/type-02/2016020001.json\n",
    "    \"\"\"\n",
    "    season = game_id[:4]\n",
    "    gtype = game_id[4:6]\n",
    "    return cache_root / season / f\"type-{gtype}\" / f\"{game_id}.json\"\n",
    "\n",
    "\n",
    "def http_get_with_retries(url: str, timeout: float = 15.0) -> Optional[requests.Response]:\n",
    "    \"\"\"\n",
    "    Perform an HTTP GET request with retry and exponential backoff.\n",
    "\n",
    "    Retries are triggered for network errors, HTTP 5xx, or HTTP 429.\n",
    "    Returns None immediately for a 404 (resource not found).\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to fetch.\n",
    "        timeout (float, optional): Timeout (seconds) for each request attempt.\n",
    "\n",
    "    Returns:\n",
    "        Optional[requests.Response]: The successful response object,\n",
    "            or None if a 404 is received or all retries fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "            if r.status_code == 404:\n",
    "                return None\n",
    "            if r.status_code >= 500 or r.status_code == 429:\n",
    "                time.sleep(BACKOFF_BASE * (2 ** attempt))\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except requests.RequestException:\n",
    "            time.sleep(BACKOFF_BASE * (2 ** attempt))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "370b868e-4af4-440a-9769-eee5e6a6f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: enumerate valid game_ids ---\n",
    "def list_game_ids_for_season(season_start_year: int, game_type: str, max_games_hint: int = 1500) -> List[str]:\n",
    "    valid_ids = []\n",
    "    miss = 0\n",
    "    for n in tqdm(range(1, max_games_hint + 1), desc=f\"{season_start_year} {game_type}\"):\n",
    "        gid = build_game_id(season_start_year, game_type, n)\n",
    "        url = f\"https://api-web.nhle.com/v1/gamecenter/{gid}/play-by-play\"\n",
    "        r = http_get_with_retries(url, timeout=TIMEOUT)\n",
    "        if r is None:\n",
    "            miss += 1\n",
    "            if miss >= 120:\n",
    "                break\n",
    "        else:\n",
    "            valid_ids.append(gid)\n",
    "            miss = 0\n",
    "        time.sleep(THROTTLE_SEC)\n",
    "    print(f\" {season_start_year} {game_type}: found {len(valid_ids)} valid games.\")\n",
    "    return valid_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5633211e-e9c4-4d3b-bd6b-6573989007d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: fetch & parallel download ---\n",
    "def fetch_one(game_id: str) -> bool:\n",
    "    p = cache_path(CACHE_ROOT, game_id)\n",
    "    if p.exists():\n",
    "        return True\n",
    "    url = f\"https://api-web.nhle.com/v1/gamecenter/{game_id}/play-by-play\"\n",
    "    r = http_get_with_retries(url, timeout=TIMEOUT)\n",
    "    if r is None:\n",
    "        return False\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(json.dumps(r.json()), encoding=\"utf-8\")\n",
    "    time.sleep(THROTTLE_SEC)\n",
    "    return True\n",
    "\n",
    "def fetch_season_fast(season_start_year: int, game_type: str):\n",
    "    game_ids = list_game_ids_for_season(season_start_year, game_type)\n",
    "    ok = 0\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futures = [ex.submit(fetch_one, gid) for gid in game_ids]\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=f\"Downloading {season_start_year} {game_type}\"):\n",
    "            try:\n",
    "                ok += 1 if f.result() else 0\n",
    "            except Exception as e:\n",
    "                print(\"[ERR]\", e)\n",
    "    print(f\" {season_start_year} {game_type}: downloaded {ok}/{len(game_ids)} games.\")\n",
    "    return ok\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d30d31-238d-46ed-9b59-e0e01013e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2016 regular ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016 regular:   1%|▎                          | 14/1500 [00:06<11:32,  2.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[ERR] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mfetch_year_range_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2016\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mregular\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplayoffs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mfetch_year_range_fast\u001b[39m\u001b[34m(start_year, end_year, types)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     ok = \u001b[43mfetch_season_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     results[(year, gtype)] = ok\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mfetch_season_fast\u001b[39m\u001b[34m(season_start_year, game_type)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_season_fast\u001b[39m(season_start_year: \u001b[38;5;28mint\u001b[39m, game_type: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     game_ids = \u001b[43mlist_game_ids_for_season\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseason_start_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     ok = \u001b[32m0\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers=MAX_WORKERS) \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mlist_game_ids_for_season\u001b[39m\u001b[34m(season_start_year, game_type, max_games_hint)\u001b[39m\n\u001b[32m      6\u001b[39m gid = build_game_id(season_start_year, game_type, n)\n\u001b[32m      7\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://api-web.nhle.com/v1/gamecenter/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/play-by-play\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m r = \u001b[43mhttp_get_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     10\u001b[39m     miss += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mhttp_get_with_retries\u001b[39m\u001b[34m(url, timeout)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_RETRIES):\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m r.status_code == \u001b[32m404\u001b[39m:\n\u001b[32m     93\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/urllib3/connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    788\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/urllib3/connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    983\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ift3700P-venv/lib/python3.12/site-packages/urllib3/util/ssl_.py:458\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m         \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    460\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def fetch_year_range_fast(start_year: int, end_year: int,\n",
    "                          types=(\"regular\", \"playoffs\")) -> dict:\n",
    "    \"\"\"\n",
    "    Download NHL play-by-play data for multiple seasons and game types in batch.\n",
    "\n",
    "    This function iterates over each season and game type, calling\n",
    "    `fetch_season_fast()` to download all available game JSONs between the given\n",
    "    start and end years.\n",
    "\n",
    "    Args:\n",
    "        start_year (int): First season to include (e.g., 2016 for the 2016–17 season).\n",
    "        end_year (int): Last season to include (inclusive).\n",
    "        types (tuple[str]): Sequence of game types to fetch, usually\n",
    "            (\"regular\", \"playoffs\").\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple[int, str], int]:\n",
    "            A dictionary mapping (season_start_year, game_type) →\n",
    "            number of successfully downloaded (or cached) games.\n",
    "\n",
    "    Notes:\n",
    "        - Each call to `fetch_season_fast()` handles retries and caching.\n",
    "        - Downloads may take significant time; consider testing first\n",
    "          with a short range (e.g. 2016–2017).\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for gtype in types:\n",
    "            print(f\"\\n=== {year} {gtype} ===\")\n",
    "            try:\n",
    "                ok = fetch_season_fast(year, gtype)\n",
    "                results[(year, gtype)] = ok\n",
    "            except Exception as e:\n",
    "                print(f\"[ERR] {year} {gtype}: {e}\")\n",
    "    return results\n",
    "\n",
    "fetch_year_range_fast(2016, 2024, types=(\"regular\", \"playoffs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a9629d8-945d-43f7-b4d6-c4600456d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: extraction to DataFrame ---\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def extract_events(json_obj: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract play-by-play events from a single NHL game JSON file.\n",
    "\n",
    "    Supports the newer API structure (api-web.nhle.com), where events\n",
    "    are stored in the top-level key `\"plays\"`.\n",
    "\n",
    "    Args:\n",
    "        json_obj (dict): Parsed JSON object representing a single game.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A table of events with at least the following columns:\n",
    "            ['event', 'secondaryType', 'period', 'periodTime', 'dateTime',\n",
    "             'team', 'x', 'y', 'eventOwnerTeamId', 'situationCode', 'empty_net']\n",
    "\n",
    "        The DataFrame may be empty if no plays are found.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    plays = json_obj.get(\"plays\", None)\n",
    "    if isinstance(plays, list) and len(plays) > 0:\n",
    "        for p in plays:\n",
    "            details = p.get(\"details\", {}) or {}\n",
    "            team    = p.get(\"team\", {}) or {}\n",
    "            periodD = p.get(\"periodDescriptor\", {}) or {}\n",
    "\n",
    "            rows.append({\n",
    "                \"event\":             p.get(\"typeDescKey\") or p.get(\"typeCode\"),\n",
    "                \"secondaryType\":     details.get(\"shotType\") or details.get(\"eventCode\"),\n",
    "                \"period\":            periodD.get(\"number\"),\n",
    "                \"periodTime\":        p.get(\"timeInPeriod\") or details.get(\"timeInPeriod\"),\n",
    "                \"dateTime\":          p.get(\"timeUTC\") or details.get(\"eventOwnerTeamTime\") or None,\n",
    "                \"team\":              team.get(\"name\") or team.get(\"triCode\"),\n",
    "                \"x\":                 details.get(\"xCoord\"),\n",
    "                \"y\":                 details.get(\"yCoord\"),\n",
    "                \n",
    "                \"eventOwnerTeamId\":  details.get(\"eventOwnerTeamId\") or p.get(\"teamId\"),\n",
    "                \"situationCode\": str(details.get(\"situationCode\") or p.get(\"situationCode\") or \"\"),\n",
    "                \"empty_net\":         details.get(\"emptyNet\") or 0,\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Return an empty DataFrame with the expected schema if no data\n",
    "    return pd.DataFrame(columns=[\n",
    "        \"event\", \"secondaryType\", \"period\", \"periodTime\",\n",
    "        \"dateTime\", \"team\", \"x\", \"y\",\n",
    "        \"eventOwnerTeamId\", \"situationCode\", \"empty_net\"\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_dataset(start: int = 2016, end: int = 2024, gtype: str = \"regular\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a combined play-by-play dataset by concatenating multiple cached game files.\n",
    "\n",
    "    Adds two columns for per-game navigation:\n",
    "      - game_id   (from filename)\n",
    "      - game_type (from folder: type-02 -> regular, type-03 -> playoffs)\n",
    "    \"\"\"\n",
    "    root = CACHE_ROOT\n",
    "    tcode = GAME_TYPE_MAP[gtype]\n",
    "    all_dfs = []\n",
    "\n",
    "    # reverse map for readability\n",
    "    rev_type = {\"01\": \"preseason\", \"02\": \"regular\", \"03\": \"playoffs\", \"04\": \"allstar\"}\n",
    "\n",
    "    for year in range(start, end + 1):\n",
    "        d = root / str(year) / f\"type-{tcode}\"\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for f in d.glob(\"*.json\"):\n",
    "            try:\n",
    "                data = json.loads(f.read_text())\n",
    "                df = extract_events(data)\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                # derive fields from path\n",
    "                game_id = f.stem\n",
    "                type_code = f.parent.name.replace(\"type-\", \"\")  # \"02\"/\"03\"\n",
    "                game_type = rev_type.get(type_code, type_code)\n",
    "                df[\"game_id\"] = game_id\n",
    "                df[\"game_type\"] = game_type\n",
    "\n",
    "                all_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(\"[WARN]\", f, e)\n",
    "\n",
    "    \n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"event\", \"secondaryType\", \"period\", \"periodTime\", \"dateTime\",\n",
    "            \"team\", \"x\", \"y\",\n",
    "            \"eventOwnerTeamId\", \"situationCode\", \"empty_net\", \n",
    "            \"game_id\", \"game_type\"\n",
    "        ])\n",
    "\n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc4f1fc5-a4d9-414e-9310-d185dddcfa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Files： 1230\n",
      "Colums： ['event', 'secondaryType', 'period', 'periodTime', 'dateTime', 'team', 'x', 'y', 'eventOwnerTeamId', 'situationCode', 'empty_net']\n",
      "Shape： (338, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>secondaryType</th>\n",
       "      <th>period</th>\n",
       "      <th>periodTime</th>\n",
       "      <th>dateTime</th>\n",
       "      <th>team</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>eventOwnerTeamId</th>\n",
       "      <th>situationCode</th>\n",
       "      <th>empty_net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>period-start</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hit</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>00:09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1551</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shot-on-goal</td>\n",
       "      <td>wrist</td>\n",
       "      <td>1</td>\n",
       "      <td>00:15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1551</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          event secondaryType  period periodTime dateTime  team     x     y  \\\n",
       "0  period-start          None       1      00:00     None  None   NaN   NaN   \n",
       "1           hit          None       1      00:09     None  None  95.0 -29.0   \n",
       "2  shot-on-goal         wrist       1      00:15     None  None  36.0 -23.0   \n",
       "\n",
       "   eventOwnerTeamId situationCode  empty_net  \n",
       "0               NaN                        0  \n",
       "1              10.0          1551          0  \n",
       "2               8.0          1551          0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "cands = list((CACHE_ROOT / \"2016\" / \"type-02\").glob(\"*.json\"))\n",
    "print(\"# Files：\", len(cands))\n",
    "sample = json.loads(cands[0].read_text())\n",
    "\n",
    "df_test = extract_events(sample)\n",
    "print(\"Colums：\", df_test.columns.tolist())\n",
    "print(\"Shape：\", df_test.shape)\n",
    "df_test.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79ef425a-27bc-4bd7-b6ff-cf404176d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: save clean dataset per year ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def save_clean_dataset(df: pd.DataFrame, out_path: Path):\n",
    "    \"\"\"\n",
    "    Save a cleaned play-by-play dataset to disk in CSV or compressed CSV (gzip) format.\n",
    "    Creates parent directories if they do not exist.\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.suffix == \".gz\" or out_path.suffixes[-2:] == [\".csv\", \".gz\"]:\n",
    "        df.to_csv(out_path, index=False, compression=\"gzip\")\n",
    "    elif out_path.suffix == \".zip\" or out_path.suffixes[-2:] == [\".csv\", \".zip\"]:\n",
    "        df.to_csv(out_path, index=False, compression=\"zip\")\n",
    "    else:\n",
    "        df.to_csv(out_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46a9c59d-7e13-446e-8da6-04c6470d801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building 2016 season dataset ===\n",
      "Shape (2016): (382015, 13)\n",
      "Saved events_regular_2016.csv.zip (3,108,818 bytes)\n",
      "Read-back OK (382015 rows, 13 cols)\n",
      "\n",
      "=== Building 2017 season dataset ===\n",
      "Shape (2017): (405453, 13)\n",
      "Saved events_regular_2017.csv.zip (3,253,562 bytes)\n",
      "Read-back OK (405453 rows, 13 cols)\n",
      "\n",
      "=== Building 2018 season dataset ===\n",
      "Shape (2018): (402616, 13)\n",
      "Saved events_regular_2018.csv.zip (3,237,609 bytes)\n",
      "Read-back OK (402616 rows, 13 cols)\n",
      "\n",
      "=== Building 2019 season dataset ===\n",
      "Shape (2019): (335749, 13)\n",
      "Saved events_regular_2019.csv.zip (2,707,439 bytes)\n",
      "Read-back OK (335749 rows, 13 cols)\n",
      "\n",
      "=== Building 2020 season dataset ===\n",
      "Shape (2020): (258119, 13)\n",
      "Saved events_regular_2020.csv.zip (2,074,997 bytes)\n",
      "Read-back OK (258119 rows, 13 cols)\n",
      "\n",
      "=== Building 2021 season dataset ===\n",
      "Shape (2021): (404656, 13)\n",
      "Saved events_regular_2021.csv.zip (3,259,757 bytes)\n",
      "Read-back OK (404656 rows, 13 cols)\n",
      "\n",
      "=== Building 2022 season dataset ===\n",
      "Shape (2022): (408732, 13)\n",
      "Saved events_regular_2022.csv.zip (3,301,440 bytes)\n",
      "Read-back OK (408732 rows, 13 cols)\n",
      "\n",
      "=== Building 2023 season dataset ===\n",
      "Shape (2023): (413676, 13)\n",
      "Saved events_regular_2023.csv.zip (3,342,302 bytes)\n",
      "Read-back OK (413676 rows, 13 cols)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start, end = 2016, 2023\n",
    "for year in range(start, end + 1):\n",
    "    print(f\"\\n=== Building {year} season dataset ===\")\n",
    "    df_year = build_dataset(year, year, \"regular\")\n",
    "    print(f\"Shape ({year}):\", df_year.shape)\n",
    "    if df_year.empty:\n",
    "        print(f\"[WARN] Year {year}: empty dataset, skipped.\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    outp = Path(f\"../data/build/events_regular_{year}.csv.zip\")\n",
    "    save_clean_dataset(df_year, outp)\n",
    "\n",
    "  \n",
    "    assert outp.exists() and outp.stat().st_size > 0, f\"Output {outp} is empty!\"\n",
    "    print(f\"Saved {outp.name} ({outp.stat().st_size:,} bytes)\")\n",
    "\n",
    " \n",
    "    df_back = pd.read_csv(outp, compression=\"zip\")\n",
    "    assert df_back.shape[0] == df_year.shape[0], f\"Row mismatch in {year}\"\n",
    "    print(f\"Read-back OK ({df_back.shape[0]} rows, {len(df_back.columns)} cols)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79581079-f721-4e8c-82ec-e6bbe8cbdc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/build/shots_features_2023.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>secondaryType</th>\n",
       "      <th>period</th>\n",
       "      <th>periodTime</th>\n",
       "      <th>dateTime</th>\n",
       "      <th>team</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>eventOwnerTeamId</th>\n",
       "      <th>situationCode</th>\n",
       "      <th>empty_net</th>\n",
       "      <th>game_id</th>\n",
       "      <th>game_type</th>\n",
       "      <th>shot_distance</th>\n",
       "      <th>shot_angle</th>\n",
       "      <th>shot_type</th>\n",
       "      <th>is_goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>period-start</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023020320</td>\n",
       "      <td>regular</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faceoff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023020320</td>\n",
       "      <td>regular</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>00:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023020320</td>\n",
       "      <td>regular</td>\n",
       "      <td>32.249031</td>\n",
       "      <td>82.874984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>giveaway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>00:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023020320</td>\n",
       "      <td>regular</td>\n",
       "      <td>34.525353</td>\n",
       "      <td>79.992020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>00:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023020320</td>\n",
       "      <td>regular</td>\n",
       "      <td>55.226805</td>\n",
       "      <td>47.935673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          event secondaryType  period periodTime  dateTime  team     x     y  \\\n",
       "0  period-start           NaN       1      00:00       NaN   NaN   NaN   NaN   \n",
       "1       faceoff           NaN       1      00:00       NaN   NaN   0.0   0.0   \n",
       "2           hit           NaN       1      00:22       NaN   NaN  93.0  32.0   \n",
       "3      giveaway           NaN       1      00:25       NaN   NaN  83.0 -34.0   \n",
       "4           hit           NaN       1      00:29       NaN   NaN  52.0 -41.0   \n",
       "\n",
       "   eventOwnerTeamId  situationCode  empty_net     game_id game_type  \\\n",
       "0               NaN         1551.0          0  2023020320   regular   \n",
       "1              24.0         1551.0          0  2023020320   regular   \n",
       "2              22.0         1551.0          0  2023020320   regular   \n",
       "3              24.0         1551.0          0  2023020320   regular   \n",
       "4              24.0         1551.0          0  2023020320   regular   \n",
       "\n",
       "   shot_distance  shot_angle shot_type  is_goal  \n",
       "0            NaN         NaN       NaN        0  \n",
       "1      89.000000    0.000000       NaN        0  \n",
       "2      32.249031   82.874984       NaN        0  \n",
       "3      34.525353   79.992020       NaN        0  \n",
       "4      55.226805   47.935673       NaN        0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate csv files\n",
    "!python3 features_basic.py --in_csv ../data/build/events_regular_2023.csv.zip --out_csv ../data/build/shots_features_2023.csv\n",
    "features = pd.read_csv(\"../data/build/shots_features_2023.csv\")\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9db0e855-ce48-443c-8328-740d496dc19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/LR_distance_angle.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"../data/build/shots_features_2023.csv\")\n",
    "\n",
    "# Target\n",
    "y = df[\"is_goal\"].astype(int)\n",
    "\n",
    "# 1) distance-only model\n",
    "X_dist = df[[\"shot_distance\"]].astype(float)\n",
    "X_dist = X_dist.dropna()\n",
    "y_dist = y.loc[X_dist.index]\n",
    "\n",
    "model_dist = LogisticRegression(max_iter=1000)\n",
    "model_dist.fit(X_dist, y_dist)\n",
    "joblib.dump(model_dist, \"../models/LR_distance_only.pkl\")\n",
    "\n",
    "# 2) angle-only model\n",
    "X_angle = df[[\"shot_angle\"]].astype(float)\n",
    "X_angle = X_angle.dropna()\n",
    "y_angle = y.loc[X_angle.index]\n",
    "\n",
    "model_angle = LogisticRegression(max_iter=1000)\n",
    "model_angle.fit(X_angle, y_angle)\n",
    "joblib.dump(model_angle, \"../models/LR_angle_only.pkl\")\n",
    "\n",
    "# 3) distance + angle model\n",
    "X_both = df[[\"shot_distance\", \"shot_angle\"]].astype(float)\n",
    "X_both = X_both.dropna()\n",
    "y_both = y.loc[X_both.index]\n",
    "\n",
    "model_both = LogisticRegression(max_iter=1000)\n",
    "model_both.fit(X_both, y_both)\n",
    "joblib.dump(model_both, \"../models/LR_distance_angle.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e0603-f7f3-47ab-b3b8-fae6a3921b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49d77e-8f81-45d1-a9a4-89ef3fb2a90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "951fd61b-c1d9-43c6-a453-e23a6226a25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"features\":[\"shot_distance\"],\"model\":\"logreg_distance\",\"n_samples\":4,\"predictions\":[0.008125526678032248,0.08189252644635488,0.07497415322416695,0.032810403443812516]}\n",
      "\n",
      "{'features': ['shot_distance'], 'model': 'logreg_distance', 'n_samples': 5, 'predictions': [0.025062140269824573, 0.024225705572491204, 0.02554758039762501, 0.025493231706241865, 0.025004111991503784]}\n"
     ]
    }
   ],
   "source": [
    "# cell for testing app.py\n",
    "import pandas as pd, requests, json\n",
    "\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "\n",
    "\n",
    "X = pd.read_csv(\"../data/build/shots_features_2023.csv\").head(5)\n",
    "payload = json.loads(X[[\"shot_distance\", \"shot_angle\"]].to_json())\n",
    "\n",
    "r = requests.post(\"http://0.0.0.0:5000/predict\", json=payload)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ae79b-aebc-4eb6-b01c-2cc842be1aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2ca86-25e7-4fc2-9805-1bd9ad2cc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Quick QA + Plot ---\n",
    "# (Run only after building df_reg / df_po)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def quick_event_counts(df: 'pd.DataFrame', topn: int = 10):\n",
    "    \"\"\"\n",
    "    Display the most frequent play-by-play event types in a dataset.\n",
    "\n",
    "    Prints the top event counts to the console and plots them as a bar chart.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset containing at least an \"event\" column.\n",
    "        topn (int): Number of top event categories to display and plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes the DataFrame contains a column named 'event'.\n",
    "        - If df is empty, the function prints a warning and returns without plotting.\n",
    "        - Uses matplotlib (no seaborn, per project guidelines).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Empty DataFrame — build or load the dataset first.\")\n",
    "        return\n",
    "    if 'event' not in df.columns:\n",
    "        print(\"Missing 'event' column — please check your extraction step.\")\n",
    "        return\n",
    "\n",
    "    counts = df['event'].value_counts().head(topn)\n",
    "    print(\"Top event counts:\\n\", counts)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    counts.plot(kind='bar')\n",
    "    plt.title(f\"Top {topn} play-by-play events\")\n",
    "    plt.xlabel(\"Event type\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb2cc7-ee70-4a4a-9271-326c0a4a7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = df_small  \n",
    "\n",
    "quick_event_counts(target_df, topn=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387734d7-e688-4821-a5b7-bccb0216c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-game interactive browser  ---\n",
    "# Browse events per game with season-type toggle and on-rink plotting.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import VBox, HBox, Dropdown, IntSlider, Output\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= 1) Load cleaned dataset (auto-detect .gz / .zip) =========\n",
    "DATA_CANDIDATES = [\n",
    "    \n",
    "    \"data/clean/events_regular_2016-2023.csv.zip\",\n",
    "     \"data/clean/events_regular_2020-2021.csv.zip\",\n",
    "    \n",
    "]\n",
    "\n",
    "def _load_any(paths):\n",
    "    for p in paths:\n",
    "        path = Path(p)\n",
    "        if path.exists():\n",
    "            comp = \"gzip\" if path.suffix == \".gz\" or path.suffixes[-2:] == [\".csv\", \".gz\"] else \\\n",
    "                   \"zip\"  if path.suffix == \".zip\" or path.suffixes[-2:] == [\".csv\", \".zip\"] else None\n",
    "            print(f\"[LOAD] {path} (compression={comp})\")\n",
    "            return pd.read_csv(path, compression=comp)\n",
    "    raise FileNotFoundError(\"No cleaned dataset found. Checked:\\n  - \" + \"\\n  - \".join(paths))\n",
    "\n",
    "path = [\n",
    "    \n",
    "    \"data/clean/events_regular_2016-2023.csv.zip\",\n",
    "    \n",
    "]\n",
    "df0 = _load_any(path)\n",
    "\n",
    "# ========= 2) Sanity checks & minimal enrich =========\n",
    "required_cols = {\n",
    "    \"event\",\"secondaryType\",\"period\",\"periodTime\",\"dateTime\",\"team\",\"x\",\"y\",\"game_id\",\"game_type\"\n",
    "}\n",
    "missing = required_cols - set(df0.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Dataset missing columns {sorted(missing)}. \"\n",
    "                     f\"Please rebuild with the updated build_dataset() that adds game_id and game_type.\")\n",
    "\n",
    "# Derive season_start_year from dateTime (Aug–Jul season logic)\n",
    "def _season_from_dt(dt_str):\n",
    "    try:\n",
    "        y = int(str(dt_str)[:4]); m = int(str(dt_str)[5:7])\n",
    "        return y if m >= 8 else (y - 1)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "if \"season_start_year\" not in df0.columns:\n",
    "    df0[\"season_start_year\"] = df0[\"dateTime\"].astype(str).map(_season_from_dt)\n",
    "\n",
    "# ========= 3) Widgets =========\n",
    "# Season options from derived season_start_year\n",
    "season_vals = sorted([int(s) for s in df0[\"season_start_year\"].dropna().unique().tolist()])\n",
    "season_dd   = Dropdown(options=[\"(all)\"] + season_vals, value=\"(all)\", description=\"Season:\")\n",
    "gtype_dd    = Dropdown(options=[\"regular\",\"playoffs\"], value=\"regular\", description=\"Type:\")\n",
    "game_dd     = Dropdown(options=[], description=\"Game ID:\")\n",
    "event_idx   = IntSlider(value=0, min=0, max=0, step=1, description=\"Event #\")\n",
    "\n",
    "out_plot = Output()\n",
    "out_text = Output()\n",
    "\n",
    "# ========= 4) Helpers =========\n",
    "def _rink(ax):\n",
    "    \"\"\"Minimal rink outline for quick plotting (not to scale).\"\"\"\n",
    "    ax.axhline(0, lw=0.5, alpha=0.3)\n",
    "    ax.set_xlim(-100, 100)\n",
    "    ax.set_ylim(-42, 42)\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_title(\"Rink\")\n",
    "\n",
    "def _refresh_games(*_):\n",
    "    sub = df0.copy()\n",
    "\n",
    "    # filter by game_type\n",
    "    if gtype_dd.value:\n",
    "        sub = sub[sub[\"game_type\"] == gtype_dd.value]\n",
    "\n",
    "    # filter by season_start_year if selected\n",
    "    if season_dd.value != \"(all)\":\n",
    "        sub = sub[sub[\"season_start_year\"] == season_dd.value]\n",
    "\n",
    "    games = sub[\"game_id\"].dropna().unique().tolist()\n",
    "    games.sort()\n",
    "\n",
    "    game_dd.options = games or [\"(none)\"]\n",
    "    game_dd.value = games[0] if games else \"(none)\"\n",
    "    _refresh_event_slider()\n",
    "\n",
    "def _refresh_event_slider(*_):\n",
    "    if game_dd.value in (None, \"(none)\"):\n",
    "        event_idx.max = 0\n",
    "        event_idx.value = 0\n",
    "        _render()\n",
    "        return\n",
    "    sub = df0[(df0[\"game_id\"] == game_dd.value)]\n",
    "    event_idx.max = max(len(sub) - 1, 0)\n",
    "    event_idx.value = 0\n",
    "    _render()\n",
    "\n",
    "def _render(*_):\n",
    "    out_plot.clear_output()\n",
    "    out_text.clear_output()\n",
    "\n",
    "    if game_dd.value in (None, \"(none)\"):\n",
    "        with out_text:\n",
    "            print(\"No game selected.\")\n",
    "        return\n",
    "\n",
    "    sub = df0[df0[\"game_id\"] == game_dd.value].reset_index(drop=True)\n",
    "    if sub.empty:\n",
    "        with out_text:\n",
    "            print(\"No events for this game.\")\n",
    "        return\n",
    "\n",
    "    # clamp index\n",
    "    i = int(event_idx.value)\n",
    "    if i < 0: i = 0\n",
    "    if i >= len(sub): i = len(sub) - 1\n",
    "\n",
    "    row = sub.iloc[i]\n",
    "\n",
    "    with out_text:\n",
    "        # Header info\n",
    "        print(f\"Game: {row['game_id']} | Type: {row.get('game_type','?')} | \"\n",
    "              f\"Season: {int(row['season_start_year']) if pd.notna(row['season_start_year']) else 'n/a'}-\"\n",
    "              f\"{str(int(row['season_start_year'])+1)[-2:] if pd.notna(row['season_start_year']) else 'n/a'}\")\n",
    "        print(f\"Event #{i+1}/{len(sub)}: {row['event']}  | Period {row['period']} @ {row['periodTime']}\")\n",
    "        print(f\"Team: {row['team']}  | dateTime: {row['dateTime']}\")\n",
    "\n",
    "        # No coordinates → print a note (as required)\n",
    "        if pd.isna(row[\"x\"]) or pd.isna(row[\"y\"]):\n",
    "            print(\"(No coordinates for this event)\")\n",
    "\n",
    "    with out_plot:\n",
    "        fig, ax = plt.subplots(figsize=(6,5))\n",
    "        _rink(ax)\n",
    "        # Draw the point if coordinates exist\n",
    "        if pd.notna(row[\"x\"]) and pd.notna(row[\"y\"]):\n",
    "            ax.scatter([row[\"x\"]], [row[\"y\"]], s=70, alpha=0.9)\n",
    "            ax.set_title(f\"{row['event']} ({row['team']})  P{int(row['period'])} {row['periodTime']}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ========= 5) Wiring & initial display =========\n",
    "season_dd.observe(_refresh_games, \"value\")\n",
    "gtype_dd.observe(_refresh_games, \"value\")\n",
    "game_dd.observe(_render, \"value\")\n",
    "event_idx.observe(_render, \"value\")\n",
    "\n",
    "_refresh_games()  # populate game list & render\n",
    "\n",
    "ui_game = VBox([\n",
    "    HBox([season_dd, gtype_dd]),\n",
    "    HBox([game_dd, event_idx]),\n",
    "    out_text,\n",
    "    out_plot\n",
    "])\n",
    "\n",
    "display(ui_game)\n",
    "\n",
    "# ========= 6)  Export this widget as a standalone HTML to embed in blog =========\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "from pathlib import Path\n",
    "Path(\"_includes\").mkdir(exist_ok=True)\n",
    "embed_minimal_html(\"_includes/interactive_widget.html\", views=[ui_game], title=\"Explorateur LNH\")\n",
    "print(\"Exported -> _includes/interactive_widget.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35d361-d4f7-4769-8dcd-f3d044132afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "\n",
    "Path(\"_includes\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "embed_minimal_html(\"_includes/interactive_widget.html\", views=[ui_game], title=\"Explorateur LNH\")\n",
    "\n",
    "print(\"OK → _includes/interactive_widget.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c1d7c-1720-40c3-b5b5-da3ed62c403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_shots_and_goals(json_obj: dict, game_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Version corrigée pour le nouveau format d'API NHL\n",
    "    Utilise eventOwnerTeamId au lieu de team\n",
    "    \"\"\"\n",
    "    plays = json_obj.get(\"plays\", [])\n",
    "    rows = []\n",
    "    \n",
    "    # Récupérer les équipes home/away pour mapper les IDs\n",
    "    home_team = json_obj.get(\"homeTeam\", {}).get(\"abbrev\")\n",
    "    away_team = json_obj.get(\"awayTeam\", {}).get(\"abbrev\")\n",
    "    home_team_id = json_obj.get(\"homeTeam\", {}).get(\"id\")\n",
    "    away_team_id = json_obj.get(\"awayTeam\", {}).get(\"id\")\n",
    "    \n",
    "    # Mapping des IDs d'équipe vers les abbreviations\n",
    "    team_id_map = {}\n",
    "    if home_team_id and home_team:\n",
    "        team_id_map[home_team_id] = home_team\n",
    "    if away_team_id and away_team:\n",
    "        team_id_map[away_team_id] = away_team\n",
    "    \n",
    "    for p in plays:\n",
    "        event = p.get(\"typeDescKey\")\n",
    "        if event not in (\"shot-on-goal\", \"goal\"):\n",
    "            continue\n",
    "\n",
    "        details = p.get(\"details\", {}) or {}\n",
    "        periodD = p.get(\"periodDescriptor\", {}) or {}\n",
    "\n",
    "        # EXTRACTION DE L'ÉQUIPE via eventOwnerTeamId\n",
    "        team_id = details.get(\"eventOwnerTeamId\")\n",
    "        team_name = team_id_map.get(team_id) if team_id else None\n",
    "        \n",
    "        # Si pas dans le mapping, utiliser l'ID directement\n",
    "        if team_name is None and team_id:\n",
    "            team_name = f\"TEAM_{team_id}\"\n",
    "\n",
    "        # Extraction des coordonnées\n",
    "        x_coord = details.get(\"xCoord\")\n",
    "        y_coord = details.get(\"yCoord\")\n",
    "        \n",
    "        # Ignorer si pas de coordonnées\n",
    "        if x_coord is None or y_coord is None:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"game_id\": game_id,\n",
    "            \"period\": periodD.get(\"number\"),\n",
    "            \"periodTime\": p.get(\"timeInPeriod\"),\n",
    "            \"team\": team_name,\n",
    "            \"eventType\": \"GOAL\" if event == \"goal\" else \"SHOT\",\n",
    "            \"shooter\": details.get(\"shooterPlayerId\"),\n",
    "            \"goalie\": details.get(\"goalieInNetId\"),\n",
    "            \"x\": x_coord,\n",
    "            \"y\": y_coord,\n",
    "            \"shotType\": details.get(\"shotType\"),\n",
    "            \"emptyNet\": details.get(\"emptyNet\", False),\n",
    "            \"strength\": details.get(\"strength\"),\n",
    "            \"dateTime\": p.get(\"timeInPeriod\"),\n",
    "            \"eventOwnerTeamId\": team_id  # Garder l'ID original aussi\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d1621-eca4-40c7-9eaa-8dbaa7cbfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_clean_shots_dataset(start=2016, end=2024, gtype=\"regular\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all shots and goals from cached JSONs into one clean DataFrame.\n",
    "    \"\"\"\n",
    "    root = CACHE_ROOT\n",
    "    tcode = GAME_TYPE_MAP[gtype]\n",
    "    dfs = []\n",
    "\n",
    "    for year in range(start, end+1):\n",
    "        d = root / str(year) / f\"type-{tcode}\"\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for f in tqdm(list(d.glob(\"*.json\")), desc=f\"{year} {gtype}\"):\n",
    "            try:\n",
    "                js = json.loads(f.read_text())\n",
    "                game_id = f.stem\n",
    "                df = extract_shots_and_goals(js, game_id)\n",
    "                if not df.empty:\n",
    "                    df[\"season_start_year\"] = year\n",
    "                    df[\"game_type\"] = gtype\n",
    "                    dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(\"[WARN]\", f, e)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "df_clean = build_clean_shots_dataset(2016, 2024, \"regular\")\n",
    "df_clean.to_csv(\"data/clean/shots_goals_regular_2016_2024.csv.zip\", index=False, compression=\"zip\")\n",
    "print(df_clean.head(10))\n",
    "print(\"Fichier sauvegardé : data/clean/shots_goals_regular_2016_2024.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b218748-1b44-438d-abb5-6ec0f512c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build cleaned shots/goals CSVs from cached JSONs for selected seasons ---\n",
    "# Input  : data/raw/<YEAR>/type-02/*.json (regular), data/raw/<YEAR>/type-03/*.json (playoffs)\n",
    "# Output : data/clean/shots_goals_<type>_<YYYY[_YYYY]>.csv.gz  (SHOT & GOAL rows only)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Config: adjust YEARS and TYPES to what you need\n",
    "# -------------------------------------------------------------------\n",
    "YEARS: List[int] = [2018, 2019, 2020]                 # e.g. [2020] for season 2020-21 only; or [2018, 2019, 2020]\n",
    "TYPES: Tuple[str, ...] = (\"regular\", \"playoffs\")     # (\"regular\",) or (\"playoffs\",) or (\"regular\",\"playoffs\")\n",
    "\n",
    "CACHE_ROOT = Path(\"data/raw\")\n",
    "CLEAN_ROOT = Path(\"data/clean\")\n",
    "CLEAN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TYPE2CODE: Dict[str, str] = {\"preseason\":\"01\",\"regular\":\"02\",\"playoffs\":\"03\",\"allstar\":\"04\"}\n",
    "CODE2TYPE: Dict[str, str] = {v:k for k,v in TYPE2CODE.items()}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Core extractors\n",
    "# -------------------------------------------------------------------\n",
    "def extract_shots_and_goals(json_obj: dict, game_id: str, game_type_label: str, season_start_year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert ONE game's play-by-play JSON (new gamecenter) into a tidy table\n",
    "    containing ONLY 'shot-on-goal' and 'goal' events.\n",
    "\n",
    "    Returned columns include (minimum per assignment):\n",
    "      - game_id, season_start_year, game_type\n",
    "      - period, periodTime\n",
    "      - team (shooting team)\n",
    "      - eventType: 'SHOT' | 'GOAL'\n",
    "      - x, y (ice coordinates; may be NaN)\n",
    "      - shooter, goalie\n",
    "      - shotType\n",
    "      - emptyNet (bool)\n",
    "      - strength (EV/PP/SH etc., often present for GOAL)\n",
    "      - dateTime (UTC if available)\n",
    "    \"\"\"\n",
    "    plays = json_obj.get(\"plays\", []) or []\n",
    "    rows = []\n",
    "    for p in plays:\n",
    "        evt = (p.get(\"typeDescKey\") or p.get(\"typeCode\") or \"\").lower()\n",
    "        if evt not in (\"shot-on-goal\", \"goal\"):\n",
    "            continue\n",
    "\n",
    "        det   = p.get(\"details\", {}) or {}\n",
    "        team  = p.get(\"team\", {}) or {}\n",
    "        perD  = p.get(\"periodDescriptor\", {}) or {}\n",
    "\n",
    "        rows.append({\n",
    "            \"game_id\":            game_id,\n",
    "            \"season_start_year\":  season_start_year,\n",
    "            \"game_type\":          game_type_label,     # 'regular' or 'playoffs'\n",
    "            \"period\":             perD.get(\"number\"),\n",
    "            \"periodTime\":         p.get(\"timeInPeriod\") or det.get(\"timeInPeriod\"),\n",
    "            \"team\":               team.get(\"triCode\") or team.get(\"name\"),\n",
    "            \"eventType\":          \"GOAL\" if evt == \"goal\" else \"SHOT\",\n",
    "            \"x\":                  det.get(\"xCoord\"),\n",
    "            \"y\":                  det.get(\"yCoord\"),\n",
    "            \"shooter\":            det.get(\"shooterName\"),\n",
    "            \"goalie\":             det.get(\"goalieName\"),\n",
    "            \"shotType\":           det.get(\"shotType\"),\n",
    "            \"emptyNet\":           bool(det.get(\"emptyNet\", False)),\n",
    "            \"strength\":           det.get(\"strength\"),\n",
    "            \"dateTime\":           p.get(\"timeUTC\") or det.get(\"eventOwnerTeamTime\"),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _cache_dir_for(year: int, game_type: str) -> Path:\n",
    "    \"\"\"Return the cache directory where raw JSON files were saved for a given season & type.\"\"\"\n",
    "    tcode = TYPE2CODE[game_type]\n",
    "    return CACHE_ROOT / str(year) / f\"type-{tcode}\"\n",
    "\n",
    "def _iter_game_files(years: Iterable[int], game_type: str) -> List[Path]:\n",
    "    \"\"\"\n",
    "    List all cached JSON files for given seasons and game type.\n",
    "    Only scans local cache; does not fetch from the web.\n",
    "    \"\"\"\n",
    "    files: List[Path] = []\n",
    "    for y in years:\n",
    "        d = _cache_dir_for(y, game_type)\n",
    "        if d.exists():\n",
    "            files.extend(sorted(d.glob(\"*.json\")))\n",
    "    return files\n",
    "\n",
    "def build_clean_for(years: Iterable[int], game_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a cleaned DataFrame (shots & goals only) for the selected seasons and one game type.\n",
    "    Reads ONLY from local cache under data/raw.\n",
    "    \"\"\"\n",
    "    files = _iter_game_files(years, game_type)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No cached JSON found for {list(years)} {game_type} under {CACHE_ROOT}/<year>/type-*/\")\n",
    "\n",
    "    dfs = []\n",
    "    for f in tqdm(files, desc=f\"Extract {game_type} ({min(years)}..{max(years)})\"):\n",
    "        try:\n",
    "            js = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "            game_id = f.stem\n",
    "            year = int(f.parent.parent.name)   # the 'year' folder name\n",
    "            df = extract_shots_and_goals(js, game_id, game_type, year)\n",
    "            if not df.empty:\n",
    "                dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN]\", f, e)\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "def _season_suffix_range(years: Iterable[int]) -> str:\n",
    "    \"\"\"\n",
    "    Build a short suffix for filenames, e.g. [2020] -> '2020_2021', [2018,2019,2020] -> '2018_2020'.\n",
    "    For a single start-year, we reflect the hockey span (e.g., 2020 -> 2020_2021).\n",
    "    \"\"\"\n",
    "    years = sorted(set(years))\n",
    "    if len(years) == 1:\n",
    "        y = years[0]\n",
    "        return f\"{y}_{y+1}\"\n",
    "    return f\"{years[0]}_{years[-1]}\"\n",
    "\n",
    "def save_clean_df(df: pd.DataFrame, out_path: Path) -> Path:\n",
    "    \"\"\"Save DataFrame as CSV or CSV.GZ based on suffix.\"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.suffix == \".gz\" or out_path.suffixes[-2:] == [\".csv\", \".zip\"]:\n",
    "        df.to_csv(out_path, index=False, compression=\"zip\")\n",
    "    else:\n",
    "        df.to_csv(out_path, index=False)\n",
    "    print(f\"[OK] Saved {out_path}  rows={len(df)}\")\n",
    "    return out_path\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Run: build per requested type, then (optionally) a merged file\n",
    "# -------------------------------------------------------------------\n",
    "saved_paths: List[Path] = []\n",
    "\n",
    "for gtype in TYPES:\n",
    "    df_clean = build_clean_for(YEARS, gtype)\n",
    "    if df_clean.empty:\n",
    "        print(f\"[WARN] Empty result for {YEARS} {gtype}\")\n",
    "        continue\n",
    "    suffix = _season_suffix_range(YEARS)\n",
    "    outp = CLEAN_ROOT / f\"shots_goals_{gtype}_{suffix}.csv.zip\"\n",
    "    save_clean_df(df_clean, outp)\n",
    "    saved_paths.append(outp)\n",
    "\n",
    "# Optional: merge all requested types (if more than one) into one file\n",
    "if len(saved_paths) >= 2:\n",
    "    merged = pd.concat([pd.read_csv(p, compression=\"zip\") for p in saved_paths], ignore_index=True)\n",
    "    outm = CLEAN_ROOT / f\"shots_goals_{_season_suffix_range(YEARS)}.csv.zip\"\n",
    "    save_clean_df(merged, outm)\n",
    "    saved_paths.append(outm)\n",
    "\n",
    "print(\"\\n[SUMMARY]\")\n",
    "for p in saved_paths:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044ee4c-5616-4090-9a44-14ac6220804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Simple visualizations for IFT6758 Étape 1 (Q4) ---\n",
    "# This cell produces:\n",
    "#   - Q1: Bar chart of total shots vs goals by shot type (single season)\n",
    "#   - Q1a: Goal probability vs distance curves for seasons 2018-19 to 2020-21\n",
    "#   - Q2: Heatmap of goal% by distance bin and shot type (single season)\n",
    "#\n",
    "# Requirements: pandas, numpy, matplotlib (no seaborn needed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "# ---------------------------\n",
    "# Config: input files & season\n",
    "# ---------------------------\n",
    "# Point this to your cleaned shots/goals dataset generated in Step 3.\n",
    "# Accepts .csv, .csv.gz or .csv.zip\n",
    "CLEAN_PATHS = [\n",
    "    \"data/clean/shots_goals_regular_2018_2020.csv.zip\",\n",
    "    \"data/clean/shots_goals_playoffs_2018_2020.csv.zip\",\n",
    "    \"data/clean/shots_goals_2018_2020.csv.zip\",\n",
    "   \n",
    "]\n",
    "\n",
    "SEASON_FOR_SINGLE = 2020  # 2020-21 season (season_start_year=2020)\n",
    "SEASONS_FOR_CURVES = [2018, 2019, 2020]  # 2018-19, 2019-20, 2020-21\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def load_first_available(paths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Try to load the first existing cleaned dataset among candidate paths.\n",
    "    Supports .gz and .zip via pandas' 'compression' parameter.\n",
    "    \"\"\"\n",
    "    for p in paths:\n",
    "        path = Path(p)\n",
    "        if path.exists():\n",
    "            comp = None\n",
    "            if path.suffix == \".gz\" or path.suffixes[-2:] == [\".csv\", \".gz\"]:\n",
    "                comp = \"gzip\"\n",
    "            elif path.suffix == \".zip\" or path.suffixes[-2:] == [\".csv\", \".zip\"]:\n",
    "                comp = \"zip\"\n",
    "            print(f\"[LOAD] {path} (compression={comp})\")\n",
    "            return pd.read_csv(path, compression=comp)\n",
    "    raise FileNotFoundError(\n",
    "        \"No cleaned dataset found. Please generate your cleaned shots/goals file first.\\n\"\n",
    "        \"Checked:\\n  - \" + \"\\n  - \".join(paths)\n",
    "    )\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize column names and types expected by this visualization cell.\n",
    "    Expected semantics:\n",
    "      - secondaryType: shot type name (if missing, fallback to 'shotType')\n",
    "      - eventType: 'SHOT' or 'GOAL' (if missing, derive from 'event')\n",
    "      - season_start_year: infer from dateTime if missing\n",
    "      - x, y present (may contain NaN)\n",
    "    \"\"\"\n",
    "   \n",
    "    if \"secondaryType\" not in df.columns and \"shotType\" in df.columns:\n",
    "        df = df.assign(secondaryType=df[\"shotType\"])\n",
    "    if \"secondaryType\" not in df.columns:\n",
    "        raise ValueError(\"Missing required column 'secondaryType' (no 'shotType' to fallback).\")\n",
    "\n",
    "    # --- eventType fallback from event ---\n",
    "    if \"eventType\" not in df.columns:\n",
    "        if \"event\" in df.columns:\n",
    "            mapped = df[\"event\"].astype(str).str.lower().map({\n",
    "                \"goal\": \"GOAL\",\n",
    "                \"shot-on-goal\": \"SHOT\",\n",
    "                \"shot\": \"SHOT\",\n",
    "            })\n",
    "            df = df.assign(eventType=mapped)\n",
    "        else:\n",
    "            raise ValueError(\"Neither 'eventType' nor 'event' found to identify SHOT/GOAL.\")\n",
    "\n",
    "    # --- coordinates presence check (allow NaN) ---\n",
    "    for c in (\"x\", \"y\"):\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing required column '{c}'.\")\n",
    "\n",
    "    # --- season inference if absent ---\n",
    "    if \"season_start_year\" not in df.columns:\n",
    "        if \"dateTime\" not in df.columns:\n",
    "            raise ValueError(\"Missing 'season_start_year' and 'dateTime' to infer it.\")\n",
    "        def _season_from_dt(dt_str):\n",
    "            try:\n",
    "                y = int(str(dt_str)[:4]); m = int(str(dt_str)[5:7])\n",
    "                return y if m >= 8 else (y - 1)\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        df[\"season_start_year\"] = df[\"dateTime\"].astype(str).map(_season_from_dt)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_distance_xy(x: pd.Series, y: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute shot distance to the attacking net using NHL rink coordinates.\n",
    "    We assume the offensive net is near x=89, y=0. Distances are in 'rink feet' units.\n",
    "    \"\"\"\n",
    "    return np.sqrt((89 - x.astype(float))**2 + (0 - y.astype(float))**2)\n",
    "\n",
    "def season_filter(df: pd.DataFrame, season_start_year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return rows for a specific NHL season (start-year), e.g., 2020 for 2020-21.\n",
    "    \"\"\"\n",
    "    return df[df[\"season_start_year\"] == season_start_year].copy()\n",
    "\n",
    "def summarize_shots_goals_by_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group by 'secondaryType' to compute total shots, total goals and goal rate.\n",
    "    Only 'SHOT' and 'GOAL' rows are considered.\n",
    "    \"\"\"\n",
    "    sub = df[df[\"eventType\"].isin([\"SHOT\", \"GOAL\"])].copy()\n",
    "    # count by (type, eventType)\n",
    "    ct = sub.groupby([\"secondaryType\", \"eventType\"]).size().unstack(fill_value=0)\n",
    "    for col in (\"SHOT\", \"GOAL\"):\n",
    "        if col not in ct.columns:\n",
    "            ct[col] = 0\n",
    "    ct[\"goal_rate\"] = ct[\"GOAL\"] / (ct[\"GOAL\"] + ct[\"SHOT\"]).replace({0: np.nan})\n",
    "    return ct.sort_values(by=[\"SHOT\", \"GOAL\"], ascending=False)\n",
    "\n",
    "def binned_goal_rate(df: pd.DataFrame, bin_width: int = 2, max_dist: int = 90) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute goal rate over distance bins for the given dataframe (SHOT & GOAL rows).\n",
    "    Returns (bin_centers, goal_rate_array).\n",
    "    \"\"\"\n",
    "    sub = df[df[\"eventType\"].isin([\"SHOT\",\"GOAL\"])].copy()\n",
    "    sub[\"distance\"] = compute_distance_xy(sub[\"x\"], sub[\"y\"])\n",
    "    bins = np.arange(0, max_dist + bin_width, bin_width)\n",
    "    labels = bins[:-1] + bin_width / 2.0\n",
    "    sub[\"d_bin\"] = pd.cut(sub[\"distance\"], bins=bins, include_lowest=True, labels=labels)\n",
    "    grp = sub.groupby(\"d_bin\")[\"eventType\"].value_counts().unstack(fill_value=0)\n",
    "    shots = grp.get(\"SHOT\", pd.Series(0, index=grp.index))\n",
    "    goals = grp.get(\"GOAL\", pd.Series(0, index=grp.index))\n",
    "    rate = (goals / (shots + goals).replace({0: np.nan})).astype(float)\n",
    "    x_centers = grp.index.astype(float)  # bin centers\n",
    "    return x_centers.values, rate.values\n",
    "\n",
    "def heatmap_goal_rate_by_distance_and_type(df: pd.DataFrame, bin_width: int = 5, max_dist: int = 90) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build a 2D matrix of goal% indexed by distance bins (rows) and shot types (columns).\n",
    "    Returns (matrix, col_labels) where matrix shape is [n_bins, n_types].\n",
    "    \"\"\"\n",
    "    sub = df[df[\"eventType\"].isin([\"SHOT\",\"GOAL\"])].copy()\n",
    "    sub[\"distance\"] = compute_distance_xy(sub[\"x\"], sub[\"y\"])\n",
    "    # distance bins\n",
    "    bins = np.arange(0, max_dist + bin_width, bin_width)\n",
    "    sub[\"d_bin\"] = pd.cut(sub[\"distance\"], bins=bins, include_lowest=True)\n",
    "    # pivot: within each (d_bin, type), compute goals / total\n",
    "    def _rate(s):\n",
    "        s = s.dropna()\n",
    "        if s.empty: return np.nan\n",
    "        goals = (s == \"GOAL\").sum()\n",
    "        total = s.size\n",
    "        return goals / total if total > 0 else np.nan\n",
    "\n",
    "    pivot = sub.pivot_table(index=\"d_bin\", columns=\"secondaryType\", values=\"eventType\", aggfunc=_rate)\n",
    "    # Sort columns alphabetically for stable display\n",
    "    pivot = pivot.reindex(sorted(pivot.columns), axis=1)\n",
    "    return pivot.values, pivot.columns.tolist(), [str(idx) for idx in pivot.index]\n",
    "\n",
    "# ---------------------------\n",
    "# Load & normalize\n",
    "# ---------------------------\n",
    "df_all = load_first_available(CLEAN_PATHS)\n",
    "\n",
    "\n",
    "if \"secondaryType\" not in df_all.columns and \"shotType\" in df_all.columns:\n",
    "    df_all[\"secondaryType\"] = df_all[\"shotType\"]\n",
    "\n",
    "df_all = ensure_columns(df_all)\n",
    "\n",
    "# ---------------------------\n",
    "# Q1: Shots vs Goals by shot type (single season)\n",
    "# ---------------------------\n",
    "df_one = season_filter(df_all, SEASON_FOR_SINGLE)\n",
    "by_type = summarize_shots_goals_by_type(df_one)\n",
    "\n",
    "print(f\"[INFO] Season {SEASON_FOR_SINGLE}-{str(SEASON_FOR_SINGLE+1)[-2:]} rows:\", len(df_one))\n",
    "display(by_type.head(10))\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Create assets directory if it doesn't exist\n",
    "Path(\"assets\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"Folder ready:\", Path('assets').resolve())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "by_type[[\"SHOT\",\"GOAL\"]].plot(kind=\"bar\", ax=plt.gca(), width=0.8)\n",
    "plt.title(f\"Shots vs Goals by Shot Type — Season {SEASON_FOR_SINGLE}-{str(SEASON_FOR_SINGLE+1)[-2:]}\")\n",
    "plt.xlabel(\"Shot type (secondaryType)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.savefig(\"Shots vs Goals by Shot Type.png\", dpi=150)\n",
    "\n",
    "\n",
    "# Optionally print quick takeaways:\n",
    "most_common = by_type[\"SHOT\"].idxmax() if not by_type.empty else \"n/a\"\n",
    "most_dangerous = by_type[\"goal_rate\"].idxmax() if \"goal_rate\" in by_type.columns and by_type[\"goal_rate\"].notna().any() else \"n/a\"\n",
    "print(f\"[TAKEAWAY] Most common shot type: {most_common}\")\n",
    "print(f\"[TAKEAWAY] Highest goal-rate shot type: {most_dangerous}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Q1a: Goal probability vs distance for 2018-19, 2019-20, 2020-21\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(9,5))\n",
    "for sy in SEASONS_FOR_CURVES:\n",
    "    df_s = season_filter(df_all, sy)\n",
    "    if df_s.empty:\n",
    "        print(f\"[WARN] No data for season_start_year={sy}\")\n",
    "        continue\n",
    "    x_mid, y_rate = binned_goal_rate(df_s, bin_width=2, max_dist=90)\n",
    "    plt.plot(x_mid, y_rate, marker=\"o\", linestyle=\"-\", label=f\"{sy}-{str(sy+1)[-2:]}\")\n",
    "plt.title(\"Goal probability vs distance (shots+goals)\\nSeasons 2018-19 to 2020-21\")\n",
    "plt.xlabel(\"Distance to net (feet)\"); plt.ylabel(\"Goal rate\")\n",
    "plt.ylim(0, 1.0); plt.legend(); plt.tight_layout(); plt.show()\n",
    "plt.savefig(\"assets/Goal probability vs distance.png\", dpi=150)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Q2: Heatmap of goal% by distance × shot type (single season)\n",
    "# ---------------------------\n",
    "mat, col_labels, row_labels = heatmap_goal_rate_by_distance_and_type(df_one, bin_width=5, max_dist=90)\n",
    "\n",
    "plt.figure(figsize=(11,6))\n",
    "im = plt.imshow(mat, aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04, label=\"Goal rate\")\n",
    "plt.xticks(ticks=np.arange(len(col_labels)), labels=col_labels, rotation=45, ha=\"right\")\n",
    "plt.yticks(ticks=np.arange(len(row_labels)), labels=row_labels)\n",
    "plt.title(f\"Goal% by distance bin and shot type — Season {SEASON_FOR_SINGLE}-{str(SEASON_FOR_SINGLE+1)[-2:]}\")\n",
    "plt.xlabel(\"Shot type\"); plt.ylabel(\"Distance bins (feet)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.savefig(\"assets/Goals by distance.png\", dpi=150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a32905-9d33-4641-a3e7-c4fe4e20a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cellule : Calcul de la densité de tir ---\n",
    "# Cette fonction calcule où les équipes tirent le plus sur la patinoire\n",
    "# Utilise la méthode KDE (estimation de densité par noyau) pour lisser les positions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "def calculer_densite_tirs(df, annee_saison, equipe=None):\n",
    "    df_saison = df[df['season_start_year'] == annee_saison].copy()\n",
    "    if equipe:\n",
    "        df_saison = df_saison[df_saison['team'] == equipe]\n",
    "\n",
    "    df_tirs = df_saison[df_saison['eventType'].isin(['SHOT', 'GOAL'])]\n",
    "    df_tirs = df_tirs.dropna(subset=['x', 'y'])\n",
    "    tirs_offensifs = df_tirs[df_tirs['x'] > 0]\n",
    "\n",
    "    if len(tirs_offensifs) == 0:\n",
    "        return None\n",
    "\n",
    "    coordonnees = tirs_offensifs[['x', 'y']].values.T\n",
    "    try:\n",
    "        kde = gaussian_kde(coordonnees)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur KDE pour {equipe or 'ligue'} → {e}\")\n",
    "        return None\n",
    "\n",
    "    x_grille = np.linspace(0, 100, 50)\n",
    "    y_grille = np.linspace(-42, 42, 50)\n",
    "    X, Y = np.meshgrid(x_grille, y_grille)\n",
    "    coord_grille = np.vstack([X.ravel(), Y.ravel()])\n",
    "    densite = kde(coord_grille).reshape(X.shape)\n",
    "\n",
    "    return {\n",
    "        'x_grille': x_grille,\n",
    "        'y_grille': y_grille,\n",
    "        'densite': densite,\n",
    "        'equipe': equipe,\n",
    "        'saison': annee_saison,\n",
    "        'total_tirs': len(tirs_offensifs)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f91b9c-800f-492c-a048-2fe8ce670f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_carte_interactive(donnees_calcul, liste_equipes):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Moyenne ligue\n",
    "    fig.add_trace(go.Contour(\n",
    "        x=donnees_calcul['moyenne_ligue']['x_grille'],\n",
    "        y=donnees_calcul['moyenne_ligue']['y_grille'],\n",
    "        z=donnees_calcul['moyenne_ligue']['densite'],\n",
    "        colorscale='Blues',\n",
    "        showscale=False,\n",
    "        name='Moyenne Ligue',\n",
    "        visible=True\n",
    "    ))\n",
    "\n",
    "    for i, equipe in enumerate(liste_equipes):\n",
    "        if equipe in donnees_calcul:\n",
    "            fig.add_trace(go.Contour(\n",
    "                x=donnees_calcul[equipe]['x_grille'],\n",
    "                y=donnees_calcul[equipe]['y_grille'],\n",
    "                z=donnees_calcul[equipe]['densite'],\n",
    "                colorscale='Reds',\n",
    "                showscale=False,\n",
    "                name=equipe,\n",
    "                visible=False\n",
    "            ))\n",
    "\n",
    "    boutons_menu = []\n",
    "\n",
    "    boutons_menu.append(dict(\n",
    "        label='Moyenne Ligue',\n",
    "        method='update',\n",
    "        args=[{'visible': [True] + [False] * len(liste_equipes)}]\n",
    "    ))\n",
    "\n",
    "    for i, equipe in enumerate(liste_equipes):\n",
    "        if equipe in donnees_calcul:\n",
    "            visibilite = [False] * (len(liste_equipes) + 1)\n",
    "            visibilite[i + 1] = True\n",
    "            boutons_menu.append(dict(\n",
    "                label=equipe,\n",
    "                method='update',\n",
    "                args=[{'visible': visibilite}]\n",
    "            ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Carte des Tirs Offensifs - Saison {donnees_calcul['moyenne_ligue']['saison']}\",\n",
    "        xaxis_title='Distance du filet (pieds)',\n",
    "        yaxis_title='Position latérale (pieds)',\n",
    "        updatemenus=[dict(\n",
    "            type=\"dropdown\",\n",
    "            direction=\"down\",\n",
    "            x=0.1,\n",
    "            y=1.15,\n",
    "            buttons=boutons_menu\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9134bd-fb7e-4f4f-af48-550261fc42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"data/graphiques\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "df_normalise = pd.read_csv(\"data/clean/shots_goals_regular_2016_2024.csv.zip\")\n",
    "saisons = sorted(df_normalise['season_start_year'].unique())\n",
    "for annee_saison in saisons:\n",
    "    print(f\"Création graphique saison {annee_saison}-{annee_saison+1}...\")\n",
    "    donnees_calcul = {}\n",
    "\n",
    "    moyenne_ligue = calculer_densite_tirs(df_normalise, annee_saison=annee_saison)\n",
    "    if moyenne_ligue is None:\n",
    "        continue\n",
    "\n",
    "    donnees_calcul['moyenne_ligue'] = moyenne_ligue\n",
    "    liste_equipes_saison = sorted([\n",
    "        team for team in df_normalise[df_normalise['season_start_year'] == annee_saison]['team'].unique()\n",
    "        if team and pd.notna(team) and team != \"UNK\"\n",
    "    ])\n",
    "\n",
    "    for equipe in liste_equipes_saison:\n",
    "        dens = calculer_densite_tirs(df_normalise, annee_saison=annee_saison, equipe=equipe)\n",
    "        if dens:\n",
    "            donnees_calcul[equipe] = dens\n",
    "\n",
    "    if len(donnees_calcul) > 1:\n",
    "        fig = creer_carte_interactive(donnees_calcul, liste_equipes_saison)\n",
    "  \n",
    "        nom_fichier = os.path.join(output_dir, f\"plan_tir_{annee_saison}_{annee_saison+1}.html\")\n",
    "        fig.write_html(nom_fichier, full_html=False, include_plotlyjs='cdn')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfd439-10d9-414e-81b3-ff4bbcc0b645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
